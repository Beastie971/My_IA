#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Fichier de compatibilit√© pour r√©soudre les erreurs d'import
√Ä utiliser temporairement pendant la migration
"""

import gradio as gr

# Variables manquantes communes
analysis_mode = "classic"  # Variable par d√©faut
mode_analysis = "classic"  # Alias
processing_mode = "standard"
enable_chunks = False
enable_global_synthesis = False
chunk_size = 3000
chunk_overlap = 200
synthesis_prompt = ""

# Dictionnaires de configuration
ANALYSIS_MODES = {
    "classic": "Analyse classique",
    "chunks": "Analyse par chunks", 
    "synthesis": "Synth√®se globale"
}

PROVIDERS = [
    "Ollama local",
    "Ollama distant", 
    "RunPod.io"
]

# Fonctions de fallback simples pour √©viter les erreurs
def on_provider_change_fn(provider):
    """Fonction de fallback pour changement de provider."""
    print(f"FALLBACK: Provider chang√© vers {provider}")
    return (
        gr.update(visible=(provider == "Ollama distant"), value=""),
        gr.update(visible=(provider == "RunPod.io"), value=""),
        gr.update(visible=(provider == "RunPod.io"), value=""),
        gr.update(value=f"Provider: {provider} (mode fallback)")
    )

def analyze_with_modes_fn(*args):
    """Fonction de fallback pour analyse."""
    print(f"FALLBACK: Fonction d'analyse appel√©e avec {len(args)} arguments")
    # Retourne le bon nombre d'√©l√©ments attendus par Gradio
    return (
        "Fonction d'analyse non disponible (mode fallback)",  # resultat_final
        "",  # stats1
        "",  # text1
        "",  # preview1
        "",  # stats2
        "",  # text2
        "",  # preview2
        "",  # current_text1
        "",  # current_text2
        "",  # current_file_path1
        "",  # current_file_path2
        "Mode fallback activ√©",  # debug_info
        ""   # chunk_report
    )

def analyze_with_chunks_fn(*args):
    """Alias pour analyse."""
    return analyze_with_modes_fn(*args)

def on_test_connection_fn(provider, ollama_url_val, runpod_endpoint, runpod_token):
    """Fonction de fallback pour test connexion."""
    print(f"FALLBACK: Test de connexion appel√© pour {provider}")
    models = ["llama2", "mistral", "codellama"]  # Mod√®les factices
    return (
        gr.update(choices=models, value=models[0]),
        gr.update(value=f"Connexion simul√©e pour {provider} (mode fallback)")
    )

def on_select_prompt_fn(name, store_dict):
    """Fonction de fallback pour s√©lection prompt."""
    print(f"FALLBACK: S√©lection prompt {name}")
    return (
        gr.update(value=f"Prompt {name} (mode fallback)"),
        gr.update(value=f"FALLBACK: Prompt {name} s√©lectionn√©")
    )

def process_files_fn(file1, file2, nettoyer, anonymiser, force_processing, processing_mode):
    """Fonction de fallback pour traitement fichiers."""
    print("FALLBACK: Traitement fichiers appel√©")
    return (
        "Traitement fichiers non disponible (mode fallback)",  # status
        "0 caract√®res",  # stats1
        "",  # preview1
        "",  # anon_report1
        "0 caract√®res",  # stats2
        "",  # preview2
        "",  # anon_report2
        "",  # current_text1
        "",  # current_text2
        "",  # current_file_path1
        "",  # current_file_path2
        "Mode fallback",  # debug_info
        ""   # chunk_report
    )

def clear_all_states_fn():
    """Fonction de fallback pour nettoyage."""
    print("FALLBACK: Nettoyage √©tats appel√©")
    return (
        "",  # unified_analysis_box
        "",  # text1_stats  
        "",  # preview1_box
        "",  # anonymization1_report
        "",  # text2_stats
        "",  # preview2_box
        "",  # anonymization2_report
        "",  # current_text1
        "",  # current_text2
        "",  # current_file_path1
        "",  # current_file_path2
        "Cache nettoy√© (mode fallback)",  # debug_prompt_box
        ""   # chunk_report_box
    )

def save_url_callback_fn(url):
    """Fonction de fallback pour sauvegarde URL."""
    print(f"FALLBACK: Sauvegarde URL {url}")
    return gr.update(value=f"URL sauvegard√©e: {url} (mode fallback)")

# Fonctions utilitaires additionnelles
def load_prompts():
    """Charge les prompts par d√©faut."""
    return {
        "Analyse juridique": "Analysez ce document juridique en d√©tail.",
        "R√©sum√©": "Faites un r√©sum√© de ce document.",
        "Points cl√©s": "Identifiez les points cl√©s de ce document."
    }

def get_default_config():
    """Configuration par d√©faut."""
    return {
        "provider": "Ollama local",
        "model": "llama2",
        "temperature": 0.7,
        "max_tokens": 2000,
        "chunk_size": 3000,
        "chunk_overlap": 200
    }

# Variables globales pour compatibilit√©
DEFAULT_PROMPTS = load_prompts()
DEFAULT_CONFIG = get_default_config()

# Export de toutes les variables et fonctions
__all__ = [
    # Fonctions principales
    'on_provider_change_fn',
    'analyze_with_modes_fn',
    'analyze_with_chunks_fn',
    'on_test_connection_fn',
    'on_select_prompt_fn',
    'process_files_fn',
    'clear_all_states_fn',
    'save_url_callback_fn',
    # Variables
    'analysis_mode',
    'mode_analysis',
    'processing_mode',
    'enable_chunks',
    'enable_global_synthesis',
    'chunk_size',
    'chunk_overlap',
    'synthesis_prompt',
    # Dictionnaires
    'ANALYSIS_MODES',
    'PROVIDERS',
    'DEFAULT_PROMPTS',
    'DEFAULT_CONFIG',
    # Utilitaires
    'load_prompts',
    'get_default_config'
]

print("‚úÖ Fichier de compatibilit√© callbacks_fix.py charg√© (version √©tendue)")
print("‚ö†Ô∏è  Toutes les fonctions sont en mode fallback")
print(f"üìù {len(__all__)} √©l√©ments export√©s")
print("üîß Variables manquantes ajout√©es : analysis_mode, mode_analysis, etc.")
