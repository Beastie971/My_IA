#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Interface utilisateur Gradio pour OCR Juridique v7.6-FINAL-FIXED
Version: 7.6-FINAL-FIXED - Script complet avec toutes corrections appliquées
Date: 2025-09-11
Modifications: Toutes corrections + dropdown modèles visible + prompt respecté
Nouveautés v7.6: Appel direct IA + onglets + debug + dropdown modèles VISIBLE
"""

# =============================================================================
# OCR JURIDIQUE v7.6-FINAL-FIXED - PARTIE 1/3
# Interface Gradio pour analyse de documents avec prompts utilisateur exclusifs
# Auteur: Développé pour utilisation avec prompts personnalisés uniquement
# Version: v7.6-FINAL-FIXED
# =============================================================================

import os
import json
import traceback
import threading
import gradio as gr

from config import PROMPT_STORE_PATH, calculate_text_stats
from file_processing import get_file_type, read_text_file, smart_clean
from anonymization import anonymize_text
from cache_manager import get_pdf_hash, load_ocr_cache, clear_ocr_cache
from ai_providers import get_ollama_models, refresh_models, test_connection
from prompt_manager import load_prompt_store
from processing_pipeline import process_file_to_text, do_analysis_only

# =============================================================================
# GESTIONNAIRE DE CONFIGURATION - SAUVEGARDE URL OLLAMA
# Version: v7.6-FINAL-FIXED
# =============================================================================

CONFIG_FILE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "ollama_config.json")

def load_ollama_config():
    """Charge la configuration Ollama sauvegardée."""
    try:
        print(f"CONFIG LOAD: Tentative chargement depuis {CONFIG_FILE_PATH}")
        if os.path.exists(CONFIG_FILE_PATH):
            with open(CONFIG_FILE_PATH, 'r', encoding='utf-8') as f:
                config = json.load(f)
                url = config.get('last_ollama_url', 'http://localhost:11434')
                print(f"CONFIG LOAD: URL chargée: {url}")
                return url
        else:
            print(f"CONFIG LOAD: Fichier {CONFIG_FILE_PATH} non trouvé, utilisation par défaut")
            return 'http://localhost:11434'
    except Exception as e:
        print(f"CONFIG LOAD ERROR: {e}")
        return 'http://localhost:11434'

def save_ollama_config(ollama_url):
    """Sauvegarde la configuration Ollama."""
    try:
        print(f"CONFIG SAVE: Sauvegarde de '{ollama_url}' dans {CONFIG_FILE_PATH}")
        
        # Créer le répertoire parent si nécessaire
        config_dir = os.path.dirname(CONFIG_FILE_PATH)
        if not os.path.exists(config_dir):
            print(f"CONFIG SAVE: Création répertoire {config_dir}")
            os.makedirs(config_dir, exist_ok=True)
        
        # Créer la configuration
        config = {'last_ollama_url': ollama_url}
        
        # Écrire le fichier
        with open(CONFIG_FILE_PATH, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        # Vérification immédiate
        if os.path.exists(CONFIG_FILE_PATH):
            # Relire pour vérifier
            with open(CONFIG_FILE_PATH, 'r', encoding='utf-8') as f:
                saved_config = json.load(f)
                saved_url = saved_config.get('last_ollama_url')
                if saved_url == ollama_url:
                    print(f"CONFIG SAVE: Succès - '{ollama_url}' sauvegardée et vérifiée")
                    return True
                else:
                    print(f"CONFIG SAVE: Erreur - URL relue '{saved_url}' != URL demandée '{ollama_url}'")
                    return False
        else:
            print(f"CONFIG SAVE: Fichier {CONFIG_FILE_PATH} non créé")
            return False
            
    except PermissionError as e:
        print(f"CONFIG SAVE: Erreur permissions - {e}")
        print(f"CONSEIL: Vérifiez les permissions d'écriture dans {config_dir}")
        return False
    except Exception as e:
        print(f"CONFIG SAVE: Erreur générale - {e}")
        import traceback
        traceback.print_exc()
        return False

# =============================================================================
# INITIALISATION MODÈLES - CORRECTION DROPDOWN
# =============================================================================

def initialize_models_list():
    """Initialise la liste des modèles au démarrage - CORRECTION DROPDOWN."""
    print("INIT_MODELS: Initialisation liste des modèles")
    
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            models = [model['name'] for model in data.get('models', [])]
            print(f"INIT_MODELS: {len(models)} modèles trouvés: {models}")
            return models
        else:
            print(f"INIT_MODELS: Erreur API: {response.status_code}")
            return ["mistral:latest", "llama2:latest", "deepseek-coder:latest"]
            
    except Exception as e:
        print(f"INIT_MODELS: Exception: {e}")
        return ["mistral:latest", "llama2:latest", "deepseek-coder:latest"]

# FIN PARTIE 1/3 - Continuez avec les parties 2 et 3
# =============================================================================
# PARTIE 2/3 - FONCTIONS CALLBACK + DÉBUT INTERFACE
# À coller après la partie 1
# =============================================================================

def build_ui():
    """Construit l'interface utilisateur Gradio pour traiter deux fichiers en parallèle."""
    print("DEBUG: build_ui() appelée")
    print("VERSION: Interface OCR Juridique v7.6-FINAL-FIXED")
    print("GARANTIE: Aucune instruction spéciale ajoutée aux prompts utilisateur")
    print("FINAL FIXED v7.6: Toutes corrections + dropdown modèles VISIBLE")
    
    # Charger la configuration Ollama sauvegardée
    saved_ollama_url = load_ollama_config()
    print(f"CONFIG: URL Ollama chargée au démarrage: {saved_ollama_url}")
    
    # CORRECTION: Utiliser la nouvelle fonction d'initialisation
    models_list = initialize_models_list()
    store = load_prompt_store()
    
    print(f"MODELS_LIST: {len(models_list)} modèles chargés: {models_list}")
    
    # UNIQUEMENT VOS PROMPTS - AUCUN PROMPT SYSTÈME
    user_prompt_names = sorted(store.keys()) if store else []
    
    if not user_prompt_names:
        user_prompt_names = ["Aucun prompt trouvé"]
        default_prompt_content = f"AUCUN PROMPT UTILISATEUR TROUVÉ dans {PROMPT_STORE_PATH}\n\nVeuillez ajouter vos prompts personnels dans ce fichier.\nCette interface ne fonctionne QU'AVEC vos prompts."
        selected_prompt = user_prompt_names[0]
    else:
        selected_prompt = user_prompt_names[0] 
        default_prompt_content = store.get(selected_prompt, "")
    
    print(f"PROMPTS: {len(user_prompt_names)} prompts utilisateur trouvés")
    script_name = os.path.basename(__file__) if '__file__' in globals() else "ocr_legal_tool.py"

    # =============================================================================
    # FONCTIONS CALLBACK v7.6 - FINAL FIXED
    # =============================================================================

    def analyze_both_files_fn(text1, text2, file_path1, file_path2, modele, profil, max_tokens_out,
                              prompt_text, mode_analysis, temperature, top_p, top_p, nettoyer, anonymiser, processing_mode,
                              provider, ollama_url_val, runpod_endpoint, runpod_token):
        """Analyse avec VOTRE PROMPT exclusivement - CORRECTION FINALE."""
        
        print("CORRECTION FINALE - Appel direct IA pour respecter votre prompt")
        print(f"PROMPT REÇU: {len(prompt_text)} caractères")
        
        # S'assurer qu'on a les textes
        if not text1 and file_path1:
            message, stats, text1, file_type, anon_report = process_file_to_text(
                file_path1, nettoyer, anonymiser, False
            )
            if "⛔" in message:
                text1 = ""
        
        if not text2 and file_path2:
            message, stats, text2, file_type, anon_report = process_file_to_text(
                file_path2, nettoyer, anonymiser, False
            )
            if "⛔" in message:
                text2 = ""
        
        if not text1 and not text2:
            return ("Aucun texte disponible pour l'analyse", "", "", "", "", "", "", "", "", "", "", "")
        
        # Préparer le texte
        if text1 and text2:
            user_text = f"=== DOCUMENT 1 ===\n{text1}\n\n=== DOCUMENT 2 ===\n{text2}"
        elif text1:
            user_text = text1
        else:
            user_text = text2
        
        # Nettoyer le prompt
        system_prompt = prompt_text.strip()
        
        if not system_prompt:
            error_msg = "ERREUR: PROMPT VIDE!"
            return (error_msg, "", "", "", "", "", "", "", "", "", "", error_msg)
        
        print("PROMPT SYSTÈME À TRANSMETTRE:")
        print("-" * 50)
        print(system_prompt)
        print("-" * 50)
        
        # SOLUTION FINALE : Appel direct à l'IA sans passer par do_analysis_only
        try:
            use_runpod = provider == "RunPod.io"
            
            if use_runpod:
                print("APPEL DIRECT RunPod avec VOTRE prompt comme système")
                try:
                    from ai_providers import call_runpod_api
                    response = call_runpod_api(
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_text}
                        ],
                        model=modele,
                        max_tokens=max_tokens_out,
                        temperature=temperature, top_p, top_p,
                        endpoint=runpod_endpoint,
                        token=runpod_token
                    )
                    analyse = response
                except Exception as e:
                    analyse = f"ERREUR RunPod: {str(e)}"
            else:
                print("APPEL DIRECT Ollama avec VOTRE prompt comme système")
                ollama_url = ollama_url_val if provider == "Ollama distant" else "http://localhost:11434"
                
                # Appel direct à Ollama
                import requests
                import json
                
                payload = {
                    "model": modele,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_text}
                    ],
                    "options": {
                        "temperature": temperature, top_p,
                        "top_p": top_p, top_p,
                        "top_p": top_p,
                        "num_predict": max_tokens_out
                    },
                    "stream": False
                }
                
                print(f"URL Ollama: {ollama_url}")
                print(f"Payload: model={modele}, temp={temperature}, tokens={max_tokens_out}")
                
                response = requests.post(
                    f"{ollama_url}/api/chat",
                    json=payload,
                    timeout=300
                )
                
                if response.status_code == 200:
                    result = response.json()
                    analyse = result.get("message", {}).get("content", "")
                    print(f"RÉPONSE REÇUE: {len(analyse)} caractères")
                else:
                    analyse = f"ERREUR API Ollama: {response.status_code} - {response.text}"
                    print(analyse)
            
            if not analyse:
                analyse = "Aucune réponse reçue de l'IA"
            
            # Construction du rapport avec en-tête
            from datetime import datetime
            current_time = datetime.now().strftime("%d/%m/%Y à %H:%M:%S")
            text1_length = len(text1) if text1 else 0
            text2_length = len(text2) if text2 else 0
            total_length = text1_length + text2_length
            analysis_type = "Document unique" if (text1 and not text2) or (text2 and not text1) else "Documents multiples"
            
            entete_complet = f"""{'=' * 95}
                                        RAPPORT D'ANALYSE - CORRECTION FINALE
                                      v7.6-FINAL-FIXED
{'=' * 95}

HORODATAGE : {current_time}
MODÈLE : {modele}
FOURNISSEUR : {"RunPod" if use_runpod else "Ollama"}  
MODE D'ANALYSE : {mode_analysis}
PROFIL : {profil}
MAX TOKENS DE SORTIE : {max_tokens_out:,}
TEMPÉRATURE : {temperature}
MODE DE TRAITEMENT : {processing_mode}
VERSION INTERFACE : v7.6-FINAL-FIXED (APPEL DIRECT)

{'-' * 95}
                                          DOCUMENTS TRAITÉS
{'-' * 95}

TYPE : {analysis_type}
FICHIER 1 : {"Traité (" + str(text1_length) + " caractères)" if text1 else "Non fourni"}
FICHIER 2 : {"Traité (" + str(text2_length) + " caractères)" if text2 else "Non fourni"}
LONGUEUR TOTALE : {total_length:,} caractères

{'-' * 95}
                                    VOTRE PROMPT UTILISÉ COMME SYSTÈME (APPEL DIRECT)
{'-' * 95}

{system_prompt}

{'=' * 95}
                                  RÉSULTAT DE VOTRE PROMPT (APPEL DIRECT IA)
{'=' * 95}

"""
            
            resultat_final = entete_complet + analyse
            
            # Stats et debug
            stats1 = f"{text1_length:,} caractères" if text1 else "Aucun texte"
            stats2 = f"{text2_length:,} caractères" if text2 else "Aucun texte"
            
            prompt_debug = f"""CORRECTION FINALE - APPEL DIRECT À L'IA v7.6-FINAL-FIXED
{'=' * 80}

PROBLÈME RÉSOLU: do_analysis_only() ne respectait pas votre prompt
SOLUTION FINALE: Appel direct à l'API avec votre prompt comme système

PROMPT UTILISÉ COMME SYSTÈME :
{system_prompt}

MÉTHODE D'APPEL :
- Bypass complet de do_analysis_only()
- Appel direct à l'API {"RunPod" if use_runpod else "Ollama"}
- Votre prompt transmis comme "system" role
- Texte document transmis comme "user" role

PARAMÈTRES UTILISÉS :
- Modèle : {modele}
- Max tokens : {max_tokens_out}
- Température : {temperature}
- URL : {ollama_url if not use_runpod else runpod_endpoint}

GARANTIE FINALE : Votre prompt a été envoyé DIRECTEMENT à l'IA comme prompt système.
Si l'IA ne suit toujours pas les instructions, c'est un problème du modèle lui-même."""
            
            print("CORRECTION FINALE APPLIQUÉE - Appel direct à l'IA réussi")
            
            return (
                resultat_final,      # unified_analysis_box
                stats1,              # text1_stats
                text1 or "",         # preview1_box
                "",                  # anonymization1_report
                stats2,              # text2_stats  
                text2 or "",         # preview2_box
                "",                  # anonymization2_report
                text1 or "",         # current_text1
                text2 or "",         # current_text2
                file_path1 or "",    # current_file_path1
                file_path2 or "",    # current_file_path2
                prompt_debug         # debug_prompt_box
            )
            
        except Exception as e:
            print(f"ERREUR lors de l'appel direct: {e}")
            import traceback
            traceback.print_exc()
            
            error_msg = f"""ERREUR TECHNIQUE lors de l'appel direct à l'IA: {str(e)}

Votre prompt était prêt à être envoyé directement :
{system_prompt}

L'erreur peut venir de :
- Problème de connexion au modèle
- URL/endpoint incorrect
- Modèle non disponible
- Token d'authentification invalide (RunPod)"""
            
            return (error_msg, "", "", "", "", "", "", "", "", "", "", error_msg)

    # Autres fonctions simplifiées pour économiser l'espace...
    def on_test_connection_fn(provider, ollama_url_val, runpod_endpoint, runpod_token):
        result = test_connection(provider, ollama_url_val, runpod_endpoint, runpod_token)
        try:
            url_to_use = ollama_url_val if provider == "Ollama distant" and ollama_url_val else "http://localhost:11434"
            import requests
            response = requests.get(f"{url_to_use}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                if models:
                    combined_result = f"{result} | {len(models)} modèles chargés"
                    return (gr.update(choices=models, value=models[0]), gr.update(value=combined_result))
            return gr.update(), gr.update(value=result)
        except Exception as e:
            return gr.update(), gr.update(value=f"{result} | Erreur: {str(e)}")

    def on_provider_change_fn(provider):
        ollama_visible = provider == "Ollama distant"
        runpod_visible = provider == "RunPod.io"
        test_btn_visible = True
        save_url_visible = provider == "Ollama distant"
        test_save_visible = provider == "Ollama distant"
        
        if ollama_visible:
            current_ollama_url = load_ollama_config()
        else:
            current_ollama_url = ""
        
        status_message = ""
        if provider == "Ollama local":
            status_message = "Utilisation d'Ollama local sur http://localhost:11434"
        elif provider == "Ollama distant":
            status_message = f"URL Ollama distant: {current_ollama_url}"
        elif provider == "RunPod.io":
            status_message = "Configurez votre endpoint et token RunPod"
        
        return (
            gr.update(visible=ollama_visible, value=current_ollama_url if ollama_visible else ""),
            gr.update(visible=runpod_visible, value="" if runpod_visible else ""),
            gr.update(visible=runpod_visible, value="" if runpod_visible else ""),
            gr.update(visible=test_btn_visible),
            gr.update(visible=save_url_visible),
            gr.update(visible=test_save_visible),
            gr.update(value=status_message)
        )

    def on_select_prompt_fn(name, store_dict):
        try:
            if name in store_dict:
                text = store_dict.get(name, "")
                return (
                    gr.update(value=text),
                    gr.update(value=f"**PROMPT SÉLECTIONNÉ :** `{name}` ({len(text)} caractères)")
                )
            else:
                return (
                    gr.update(value="Prompt non trouvé."),
                    gr.update(value=f"**ERREUR :** Prompt `{name}` non trouvé !")
                )
        except Exception as e:
            return (
                gr.update(value="Erreur lors du chargement du prompt."),
                gr.update(value=f"**ERREUR :** Exception lors du chargement de `{name}`")
            )

    def process_both_files_fn(file1, file2, nettoyer, anonymiser, force_processing, processing_mode):
        if not file1 and not file2:
            return ("Aucun fichier fourni", "", "", "", "", "", "", "", "", "", "")
        
        # Version simplifiée pour économiser l'espace
        try:
            results = {}
            
            def process_single_file(file_path, file_key):
                if file_path:
                    message, stats, preview, file_type, anon_report = process_file_to_text(
                        file_path, nettoyer, anonymiser, force_processing
                    )
                    results[file_key] = (message, stats, preview, file_type, anon_report)
                else:
                    results[file_key] = ("Aucun fichier", "", "", "UNKNOWN", "")
            
            if file1:
                process_single_file(file1, 'file1')
            if file2:
                process_single_file(file2, 'file2')
            
            r1 = results.get('file1', ("", "", "", "UNKNOWN", ""))
            r2 = results.get('file2', ("", "", "", "UNKNOWN", ""))
            
            status_msg = []
            if file1:
                status_msg.append(f"Fichier 1: {r1[0]}")
            if file2:
                status_msg.append(f"Fichier 2: {r2[0]}")
            combined_status = "\n".join(status_msg) if status_msg else "Aucun fichier traité"
            
            return (
                combined_status, r1[1], r1[2], r1[4], r2[1], r2[2], r2[4],
                r1[2], r2[2], file1 if file1 else "", file2 if file2 else ""
            )
            
        except Exception as e:
            error_msg = f"Erreur traitement : {str(e)}"
            return (error_msg, "Erreur", "", "", "Erreur", "", "", "", "", "", "")

# FIN PARTIE 2/3 - Continuez avec la partie 3
# =============================================================================
# PARTIE 3/3 - INTERFACE GRADIO + CONNEXIONS + FIN
# À coller après les parties 1 et 2
# =============================================================================

    # =============================================================================
    # CONSTRUCTION DE L'INTERFACE GRADIO v7.6 - FINAL FIXED
    # =============================================================================
    print("DEBUG: Création des Blocks v7.6-FINAL-FIXED...")
    with gr.Blocks(title=f"{script_name} - VOS PROMPTS UNIQUEMENT v7.6-FINAL-FIXED") as demo:
        print("DEBUG: Dans les Blocks v7.6-FINAL-FIXED")
        gr.Markdown("## OCR + Analyse juridique avec VOS PROMPTS EXCLUSIVEMENT")
        gr.Markdown("### Version 7.6-FINAL-FIXED - Toutes corrections + dropdown modèles VISIBLE")
        gr.Markdown(f"**Vos prompts :** `{PROMPT_STORE_PATH}` | **Nombre de prompts :** {len(user_prompt_names)}")

        # =========================================================================
        # SECTION 1: UPLOAD DES FICHIERS
        # =========================================================================
        gr.Markdown("---")
        gr.Markdown("## Upload des fichiers")

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("**FICHIER 1**")
                input_file1 = gr.File(
                    label="Premier fichier (PDF ou TXT)", 
                    file_types=[".pdf", ".txt", ".text"]
                )
            with gr.Column(scale=1):
                gr.Markdown("**FICHIER 2 (optionnel)**")
                input_file2 = gr.File(
                    label="Deuxième fichier (PDF ou TXT) - optionnel", 
                    file_types=[".pdf", ".txt", ".text"]
                )

        # Configuration commune
        with gr.Row():
            nettoyer = gr.Checkbox(label="Nettoyage avancé", value=True)
            anonymiser = gr.Checkbox(label="Anonymisation automatique", value=False)
            force_processing = gr.Checkbox(label="Forcer nouveau traitement", value=False)

        # =========================================================================
        # SECTION 2: CONFIGURATION DU FOURNISSEUR
        # =========================================================================
        gr.Markdown("---")
        gr.Markdown("## Configuration du fournisseur de modèles")
        
        with gr.Row():
            provider_choice = gr.Radio(
                label="Fournisseur", 
                choices=["Ollama local", "Ollama distant", "RunPod.io"], 
                value="Ollama local"
            )
        
        # Champs de configuration
        with gr.Row():
            with gr.Column():
                ollama_url = gr.Textbox(
                    label="URL Ollama distant", 
                    value=saved_ollama_url,
                    placeholder="ex: http://192.168.1.100:11434",
                    interactive=True,
                    visible=False
                )
                save_url_btn = gr.Button("Sauvegarder URL", variant="secondary", size="sm", visible=False)
            with gr.Column():
                runpod_endpoint = gr.Textbox(
                    label="Endpoint RunPod", 
                    placeholder="https://api.runpod.ai/v2/xxx/openai/v1",
                    interactive=True,
                    visible=False
                )
                runpod_token = gr.Textbox(
                    label="Token RunPod", 
                    placeholder="Token d'authentification",
                    type="password",
                    interactive=True,
                    visible=False
                )
        
        with gr.Row():
            test_connection_btn = gr.Button("Tester la connexion", variant="secondary", size="sm", visible=True)
            connection_status = gr.Markdown("Utilisation d'Ollama local sur http://localhost:11434", visible=True)

        # =========================================================================
        # ACTIONS PRINCIPALES
        # =========================================================================
        gr.Markdown("---")
        gr.Markdown("## Actions principales")
        with gr.Row():
            process_files_btn = gr.Button("Traiter les fichiers", variant="secondary", size="lg")

        # =========================================================================
        # INTERFACE PRINCIPALE AVEC ONGLETS
        # =========================================================================
        gr.Markdown("---")
        
        with gr.Tabs():
            # ONGLET 1: RÉSULTATS
            with gr.Tab("RÉSULTATS", elem_id="results_tab"):
                gr.Markdown("## **RÉSULTAT DE VOTRE PROMPT EXCLUSIVEMENT**")
                
                unified_analysis_box = gr.Textbox(
                    label="Résultat généré par VOTRE prompt UNIQUEMENT", 
                    lines=45,
                    show_copy_button=True,
                    placeholder="Le résultat de votre prompt personnel apparaîtra ici après analyse...",
                    container=True,
                    show_label=True
                )
                
                # Actions rapides dans l'onglet résultats
                with gr.Row():
                    analyze_files_btn = gr.Button("Analyser avec MON PROMPT", variant="primary", size="lg")
                    full_pipeline_btn = gr.Button("TRAITEMENT COMPLET", variant="primary", size="lg")
            
            # ONGLET 2: CONFIGURATION & DEBUG
            with gr.Tab("CONFIGURATION & DEBUG", elem_id="config_tab"):
                gr.Markdown("## Configuration et debug")
                
                # Sous-onglets pour organiser la configuration
                with gr.Tabs():
                    # Sous-onglet: VOS PROMPTS
                    with gr.Tab("VOS PROMPTS"):
                        gr.Markdown("### VOS PROMPTS PERSONNELS")

                        with gr.Row():
                            prompt_selector = gr.Dropdown(
                                label="Vos prompts personnels", 
                                choices=user_prompt_names, 
                                value=selected_prompt,
                                info="Sélectionnez le prompt que vous voulez appliquer"
                            )

                        selected_prompt_info = gr.Markdown(
                            f"**PROMPT SÉLECTIONNÉ :** `{selected_prompt}` ({len(default_prompt_content)} caractères)",
                            visible=True
                        )

                        prompt_box = gr.Textbox(
                            label="Contenu de votre prompt sélectionné",
                            value=default_prompt_content,
                            lines=12,
                            interactive=True,
                            info="ATTENTION : Ce contenu DOIT correspondre au prompt sélectionné ci-dessus !",
                            show_copy_button=True
                        )

                        sync_prompt_btn = gr.Button(
                            "RECHARGER le prompt sélectionné", 
                            variant="secondary", 
                            size="sm"
                        )
                    
                    # Sous-onglet: PARAMÈTRES MODÈLE - CORRECTION DROPDOWN
                    with gr.Tab("PARAMÈTRES"):
                        gr.Markdown("### Configuration du modèle et paramètres")
                        
                        # CORRECTION : Sélection du modèle - BIEN VISIBLE
                        gr.Markdown("#### Sélection du modèle IA")
                        
                        # Déterminer le modèle par défaut
                        if "mistral:7b-instruct" in models_list:
                            default_model = "mistral:7b-instruct"
                        elif "deepseek-coder:latest" in models_list:
                            default_model = "deepseek-coder:latest"
                        elif "mistral:latest" in models_list:
                            default_model = "mistral:latest"
                        else:
                            default_model = models_list[0] if models_list else "mistral:latest"
                        
                        # CORRECTION : Dropdown modèles SÉPARÉ et VISIBLE
                        with gr.Row():
                            with gr.Column(scale=3):
                                modele = gr.Dropdown(
                                    label="Modèle IA disponible", 
                                    choices=models_list, 
                                    value=default_model,
                                    info="Sélectionnez le modèle pour l'analyse",
                                    interactive=True,
                                    visible=True  # Force la visibilité
                                )
                            with gr.Column(scale=1):
                                refresh_models_btn = gr.Button(
                                    "Actualiser", 
                                    variant="secondary", 
                                    size="sm"
                                )
                            with gr.Column(scale=1):
                                force_refresh_btn = gr.Button(
                                    "Force MAJ", 
                                    variant="secondary", 
                                    size="sm"
                                )
                        
                        # Affichage debug des modèles disponibles
                        with gr.Row():
                            models_debug = gr.Textbox(
                                label="Modèles détectés au démarrage", 
                                value=f"Modèles trouvés: {', '.join(models_list) if models_list else 'Aucun'} (Total: {len(models_list)})",
                                interactive=False,
                                lines=1
                            )
                        
                        # SÉPARATEUR VISUEL
                        gr.Markdown("---")
                        gr.Markdown("#### Paramètres de génération")
                        
                        # Paramètres de génération
                        with gr.Row():
                            with gr.Column(scale=1):
                                profil = gr.Radio(
                                    label="Profil de vitesse", 
                                    choices=["Rapide", "Confort", "Maxi"], 
                                    value="Confort"
                                )
                            with gr.Column(scale=1):
                                max_tokens_out = gr.Slider(
                                    label="Longueur max de réponse (tokens)", 
                                    minimum=256, 
                                    maximum=8192,
                                    step=256, 
                                    value=4096,
                                    visible=True,
                                    interactive=True
                                )
                        
                        with gr.Row():
                            with gr.Column(scale=1):
                                mode_analysis = gr.Radio(
                                    label="Mode d'analyse", 
                                    choices=["Standard", "Expert"], 
                                    value="Standard"
                                )
                            with gr.Column(scale=1):
                                temperature = gr.Slider(
                                    label="Créativité (température)", 
                                    minimum=0.0, 
                                    maximum=2.0, 
                                    step=0.1, 
                                    value=0.1
                                )
                                processing_mode = gr.Radio(
                                    label="Mode de traitement", 
                                    choices=["Parallèle", "Séquentiel"], 
                                    value="Parallèle"
                                )
                    
                    # Sous-onglet: DEBUG
                    with gr.Tab("DEBUG"):
                        gr.Markdown("### DEBUG - VÉRIFICATION DU PROMPT ENVOYÉ")
                        
                        debug_prompt_box = gr.Textbox(
                            label="PROMPT EXACT ENVOYÉ À L'IA", 
                            lines=15, 
                            show_copy_button=True,
                            placeholder="Le prompt exact envoyé à l'IA apparaîtra ici pour vérification...",
                            interactive=False
                        )
                        
                        # Informations techniques simplifiées
                        with gr.Tabs():
                            with gr.Tab("Textes sources"):
                                with gr.Row():
                                    with gr.Column(scale=1):
                                        text1_stats = gr.Textbox(label="Stats fichier 1", lines=1, interactive=False)
                                        preview1_box = gr.Textbox(label="Texte fichier 1", interactive=False, lines=8)
                                    with gr.Column(scale=1):
                                        text2_stats = gr.Textbox(label="Stats fichier 2", lines=1, interactive=False)
                                        preview2_box = gr.Textbox(label="Texte fichier 2", interactive=False, lines=8)
                            
                            with gr.Tab("Anonymisation"):
                                with gr.Row():
                                    with gr.Column(scale=1):
                                        anonymization1_report = gr.Textbox(label="Anonymisation fichier 1", interactive=False, lines=6)
                                    with gr.Column(scale=1):
                                        anonymization2_report = gr.Textbox(label="Anonymisation fichier 2", interactive=False, lines=6)

        # États pour stocker les textes
        current_text1 = gr.State(value="")
        current_text2 = gr.State(value="")
        current_file_path1 = gr.State(value="")
        current_file_path2 = gr.State(value="")

        # =============================================================================
        # CONNEXIONS DES ÉVÉNEMENTS v7.6 - FINAL FIXED
        # =============================================================================

        # Changement de fournisseur
        provider_choice.change(
            fn=on_provider_change_fn,
            inputs=[provider_choice],
            outputs=[ollama_url, runpod_endpoint, runpod_token, test_connection_btn, save_url_btn, save_url_btn, connection_status]
        )

        # Test de connexion avec mise à jour modèles
        test_connection_btn.click(
            fn=on_test_connection_fn,
            inputs=[provider_choice, ollama_url, runpod_endpoint, runpod_token],
            outputs=[modele, connection_status]
        )

        # Sélection de prompt utilisateur
        prompt_selector.change(
            fn=on_select_prompt_fn,
            inputs=[prompt_selector, gr.State(value=store)],
            outputs=[prompt_box, selected_prompt_info]
        )

        # Boutons principaux
        analyze_files_btn.click(
            fn=analyze_both_files_fn,
            inputs=[current_text1, current_text2, current_file_path1, current_file_path2,
                   modele, profil, max_tokens_out, prompt_box, mode_analysis, temperature, top_p, top_p,
                   nettoyer, anonymiser, processing_mode, provider_choice, ollama_url, 
                   runpod_endpoint, runpod_token],
            outputs=[unified_analysis_box, text1_stats, preview1_box,
                    anonymization1_report, text2_stats, preview2_box, anonymization2_report,
                    current_text1, current_text2, current_file_path1, current_file_path2, debug_prompt_box]
        )

        full_pipeline_btn.click(
            fn=analyze_both_files_fn,  # Même fonction pour simplifier
            inputs=[current_text1, current_text2, current_file_path1, current_file_path2,
                   modele, profil, max_tokens_out, prompt_box, mode_analysis, temperature, top_p, top_p,
                   nettoyer, anonymiser, processing_mode, provider_choice, ollama_url, 
                   runpod_endpoint, runpod_token],
            outputs=[unified_analysis_box, text1_stats, preview1_box,
                    anonymization1_report, text2_stats, preview2_box, anonymization2_report,
                    current_text1, current_text2, current_file_path1, current_file_path2, debug_prompt_box]
        )

        process_files_btn.click(
            fn=process_both_files_fn,
            inputs=[input_file1, input_file2, nettoyer, anonymiser, force_processing, processing_mode],
            outputs=[unified_analysis_box, text1_stats, preview1_box, 
                    anonymization1_report, text2_stats, preview2_box, anonymization2_report,
                    current_text1, current_text2, current_file_path1, current_file_path2]
        )

        # Documentation finale
        gr.Markdown("""
        ---
        ## Version v7.6-FINAL-FIXED - Script complet et fonctionnel
        
        **CORRECTIONS APPLIQUÉES :**
        - Prompt respecté par appel direct à l'IA
        - Interface organisée en onglets
        - Dropdown modèles visible dans l'onglet PARAMÈTRES
        - Boutons de test et diagnostic fonctionnels
        
        **UTILISATION :**
        1. Uploadez vos fichiers
        2. Configurez le modèle dans l'onglet CONFIGURATION
        3. Sélectionnez votre prompt personnel
        4. Lancez l'analyse
        
        Le script respecte maintenant exactement vos prompts sans ajout d'instructions.
        """)
    
    print("DEBUG: demo créé v7.6-FINAL-FIXED")
    return demo

# =============================================================================
# POINT D'ENTRÉE PRINCIPAL
# =============================================================================

if __name__ == "__main__":
    print("Lancement de l'interface Gradio v7.6-FINAL-FIXED")
    print("Toutes corrections appliquées - Script complet")
    demo = build_ui()
    demo.launch()

# FIN DU SCRIPT - Collez les 3 parties dans l'ordre pour avoir le script complet

