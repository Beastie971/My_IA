#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
API Ollama et RunPod pour OCR Juridique v7.2-SYSTEM-PROMPT-FIXED
Version: 7.2-SYSTEM-PROMPT-FIXED - Assure la bonne s√©paration syst√®me/utilisateur
Date: 2025-01-04
"""

import json
import requests
import gradio as gr

# =============================================================================
# API OLLAMA/RUNPOD AVEC S√âPARATION SYST√àME/UTILISATEUR CLAIRE
# =============================================================================

def generate_with_ollama(model: str, system_prompt: str, user_text: str,
                         num_ctx: int, num_predict: int, temperature: float = 0.2,
                         timeout: int = 900, ollama_url: str = "http://localhost:11434") -> str:
    """G√©n√®re une r√©ponse avec Ollama - Version avec s√©paration syst√®me/utilisateur et GPU optimis√©."""
    # Calculer un timeout intelligent si non sp√©cifi√©
    if timeout is None:
        timeout = calculate_smart_timeout(len(user_text), model)
        print(f"Timeout calcul√© automatiquement : {timeout}s")
    
    url = f"{ollama_url}/api/chat"
    
    # S√âPARATION CLAIRE : prompt syst√®me vs texte utilisateur
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_text}
        ],
        "options": {
            "temperature": temperature,
            "top_p": 0.9,
            "num_predict": num_predict,
            "num_gpu": -1,  # üî• FORCE UTILISATION DE TOUS LES GPUs DISPONIBLES
            "num_batch": 512,  # Augmenter la taille de batch
            "num_thread": 8,  # Parall√©lisation CPU
            "repeat_penalty": 1.1,
            "num_ctx": num_ctx,
            "use_mmap": True,  # Optimisation m√©moire
            "use_mlock": True,  # Verrouillage m√©moire pour √©viter le swap
            "low_vram": False  # üî• MODE HAUTE PERFORMANCE GPU
        },
       "stream": False
    }
    
    print(f"ü§ñ Ollama - Prompt syst√®me : {len(system_prompt)} caract√®res")
    print(f"üìÑ Ollama - Texte utilisateur : {len(user_text)} caract√®res")
    print(f"üî• GPU forc√© avec tous les GPUs disponibles (num_gpu=-1)")
    
    try:
        response = requests.post(url, json=payload, timeout=timeout)
    except requests.exceptions.ConnectionError:
        return f"‚ùå Erreur : Impossible de se connecter √† Ollama ({ollama_url}). V√©rifiez qu'Ollama est d√©marr√© (ollama serve)."
    except requests.exceptions.Timeout:
        return f"‚ùå Erreur : D√©lai d√©pass√© ({timeout}s)."
    except Exception as e:
        return f"‚ùå Erreur de connexion Ollama : {e}"
    
    if response.status_code != 200:
        error_text = response.text
        if "system memory" in error_text.lower() and "available" in error_text.lower():
            return f"‚ùå M√âMOIRE INSUFFISANTE : Le mod√®le n√©cessite plus de RAM que disponible.\n\n" \
                   f"Solutions :\n" \
                   f"1. Utilisez un mod√®le plus l√©ger (mistral:7b-instruct ou deepseek-coder:latest)\n" \
                   f"2. Fermez d'autres applications pour lib√©rer de la RAM\n" \
                   f"3. Red√©marrez Ollama : 'ollama serve'\n\n" \
                   f"Erreur compl√®te : {error_text}"
        return f"‚ùå Erreur HTTP {response.status_code} : {error_text}"

    try:
        data = response.json()
        if "message" in data and "content" in data["message"]:
            return data["message"]["content"]
        elif "error" in data:
            return f"‚ùå Erreur Ollama : {data['error']}"
        else:
            return f"‚ùå R√©ponse inattendue : {data}"
    except json.JSONDecodeError:
        return f"‚ùå Erreur de d√©codage JSON : {response.text[:500]}"
    except Exception as e:
        return f"‚ùå Erreur lors de la lecture de la r√©ponse : {e}"

def generate_with_runpod(model: str, system_prompt: str, user_text: str,
                        num_ctx: int, num_predict: int, temperature: float = 0.2,
                        timeout: int = 900, endpoint: str = "", token: str = "") -> str:
    """G√©n√®re une r√©ponse avec RunPod API - Version avec s√©paration syst√®me/utilisateur."""
    if not endpoint or not token:
        return "‚ùå Endpoint et token RunPod requis"
    
    url = f"{endpoint}/v1/chat/completions"
    
    # S√âPARATION CLAIRE : messages syst√®me vs utilisateur
    messages = [
        {"role": "system", "content": system_prompt},  # Prompt syst√®me (instructions)
        {"role": "user", "content": user_text}         # Texte utilisateur
    ]
    
    payload = {
        "model": model,
        "messages": messages,
        "max_tokens": num_predict,
        "temperature": temperature,
        "stream": False
    }
    
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }
    
    print(f"ü§ñ RunPod - Prompt syst√®me : {len(system_prompt)} caract√®res")
    print(f"üìÑ RunPod - Texte utilisateur : {len(user_text)} caract√®res")
    
    try:
        response = requests.post(url, json=payload, headers=headers, timeout=timeout)
        
        if response.status_code != 200:
            return f"‚ùå Erreur RunPod HTTP {response.status_code} : {response.text}"
        
        data = response.json()
        
        if "choices" in data and len(data["choices"]) > 0:
            return data["choices"][0]["message"]["content"]
        else:
            return f"‚ùå R√©ponse RunPod invalide : {data}"
            
    except requests.exceptions.Timeout:
        return f"‚ùå Erreur : D√©lai d√©pass√© ({timeout}s)."
    except Exception as e:
        return f"‚ùå Erreur RunPod : {e}"

# =============================================================================
# FONCTIONS UTILITAIRES INCHANG√âES
# =============================================================================

def get_ollama_models(ollama_url: str = "http://localhost:11434"):
    """R√©cup√®re la liste des mod√®les Ollama disponibles."""
    fallback_models = [
        "mistral:7b-instruct",
        "mistral:latest", 
        "deepseek-coder:latest",
        "llama3:latest",
        "llama3.1:8b-instruct-q5_K_M"
    ]
    
    try:
        print(f"üîç Tentative de r√©cup√©ration des mod√®les Ollama depuis {ollama_url}...")
        r = requests.get(f"{ollama_url}/api/tags", timeout=10)
        
        if r.status_code != 200:
            print(f"‚ùå Erreur API Ollama - Status {r.status_code}: {r.text}")
            return fallback_models
        
        data = r.json()
        models = data.get("models", [])
        names = []
        
        for m in models:
            name = m.get("name")
            if name:
                names.append(name)
        
        if not names:
            print("‚ö†Ô∏è Aucun mod√®le trouv√© dans la r√©ponse, utilisation des mod√®les de fallback")
            return fallback_models
        
        print(f"‚úÖ Total: {len(names)} mod√®le(s) r√©cup√©r√©(s) depuis l'API")
        return names
        
    except requests.exceptions.ConnectionError:
        print(f"‚ùå Ollama non accessible sur {ollama_url} (connexion refus√©e)")
        print("V√©rifiez qu'Ollama est d√©marr√© avec: ollama serve")
        return fallback_models
    except Exception as e:
        print(f"‚ùå Erreur r√©cup√©ration mod√®les: {e}")
        return fallback_models

def refresh_models(provider, ollama_url):
    """Actualise la liste des mod√®les selon le fournisseur."""
    if provider == "Ollama local":
        models = get_ollama_models("http://localhost:11434")
    elif provider == "Ollama distant":
        if not ollama_url.strip():
            ollama_url = "http://localhost:11434"
        models = get_ollama_models(ollama_url)
    elif provider == "RunPod.io":
        models = [
            "meta-llama/Llama-3.1-70B-Instruct",
            "mistralai/Mistral-7B-Instruct-v0.3",
            "microsoft/DialoGPT-large",
            "NousResearch/Nous-Hermes-2-Yi-34B",
            "meta-llama/Llama-3.2-3B-Instruct",
            "Qwen/Qwen2.5-72B-Instruct"
        ]
    else:
        models = ["Erreur: Fournisseur inconnu"]
    
    info = f"Mod√®les actualis√©s pour {provider}"
    if provider == "Ollama distant":
        info += f" ({ollama_url})"
    info += f" : {len(models)} mod√®le(s) trouv√©(s)"
    
    return gr.update(choices=models, value=models[0] if models else ""), info

def validate_ollama_url(url: str) -> tuple:
    """Valide une URL Ollama et teste la connexion."""
    if not url.strip():
        return False, "URL vide"
    
    # Ajouter http:// si manquant
    if not url.startswith(('http://', 'https://')):
        url = f"http://{url}"
    
    try:
        import requests
        response = requests.get(f"{url}/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            model_count = len(data.get("models", []))
            return True, f"Connexion r√©ussie - {model_count} mod√®le(s) disponible(s)"
        else:
            return False, f"Erreur HTTP {response.status_code}"
    except requests.exceptions.ConnectionError:
        return False, "Impossible de se connecter - v√©rifiez que le serveur Ollama est d√©marr√©"
    except requests.exceptions.Timeout:
        return False, "D√©lai de connexion d√©pass√©"
    except Exception as e:
        return False, f"Erreur : {str(e)}"

def test_connection(provider, ollama_url, runpod_endpoint, runpod_token):
    """Test la connexion selon le fournisseur s√©lectionn√©."""
    if provider == "Ollama local":
        success, message = validate_ollama_url("http://localhost:11434")
        return f"Test connexion Ollama local : {message}"
    
    elif provider == "Ollama distant":
        if not ollama_url.strip():
            return "Test connexion : URL Ollama distante requise"
        success, message = validate_ollama_url(ollama_url)
        return f"Test connexion Ollama distant : {message}"
    
    elif provider == "RunPod.io":
        if not runpod_endpoint.strip() or not runpod_token.strip():
            return "Test connexion : Endpoint et token RunPod requis"
        
        try:
            import requests
            headers = {
                "Authorization": f"Bearer {runpod_token}",
                "Content-Type": "application/json"
            }
            # Test simple sans vraie requ√™te pour √©viter les co√ªts
            url = f"{runpod_endpoint}/v1/models"
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code in [200, 404]:  # 404 acceptable pour certains endpoints
                return "Test connexion RunPod : Connexion r√©ussie"
            else:
                return f"Test connexion RunPod : Erreur HTTP {response.status_code}"
        except Exception as e:
            return f"Test connexion RunPod : Erreur - {str(e)}"
    
    return "Test connexion : Fournisseur non reconnu"

def calculate_smart_timeout(text_length: int, model: str) -> int:
    """Calcule un timeout intelligent bas√© sur la longueur du texte et le mod√®le."""
    base_timeout = 60
    text_factor = max(1, text_length // 1000)  # 1 seconde par 1000 caract√®res
    
    # Facteur mod√®le (certains mod√®les sont plus lents)
    model_factors = {
        "llama": 2,
        "mistral": 1.5,
        "deepseek": 1.3,
        "default": 1
    }
    
    model_factor = model_factors.get("default", 1)
    for key, factor in model_factors.items():
        if key in model.lower():
            model_factor = factor
            break
    
    timeout = int(base_timeout + (text_factor * model_factor))
    return min(timeout, 1800)  # Maximum 30 minutes
