#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Interface Gradio OCR Juridique - VERSION COMPL√àTE AVEC MODULES HYBRIDES
Version: 8.9-COMPLETE-HYBRID
Date: 2025-09-17
OBJECTIF: Interface compl√®te avec analyse hybride + analyse simple int√©gr√©es
"""

import os
import gradio as gr
import json
import time
import requests
from datetime import datetime
import threading

# ========================================
# IMPORTS DES MODULES EXISTANTS
# ========================================

# Modules de base (d√©j√† fonctionnels)
from processing_pipeline import process_file_to_text
from ai_providers import generate_with_ollama, generate_with_runpod, get_ollama_models
from config import DEFAULT_PROMPT_TEXT, EXPERT_PROMPT_TEXT

# Modules avanc√©s (√† int√©grer)
try:
    from chunck_analysis import ChunkAnalyzer
    from ai_wrapper import ai_call_wrapper, test_ai_connection, validate_ai_params
    CHUNK_ANALYSIS_AVAILABLE = True
    print("‚úÖ Module d'analyse par chunks disponible")
except ImportError as e:
    print(f"‚ö†Ô∏è Module d'analyse par chunks non disponible: {e}")
    CHUNK_ANALYSIS_AVAILABLE = False

# Tentative d'import du module hybride
try:
    # Si vous avez un module hybrid_analyzer
    from hybrid_analyzer import HybridAnalyzer, create_hybrid_analyzer
    HYBRID_AVAILABLE = True
    print("‚úÖ Module d'analyse hybride disponible")
except ImportError:
    print("‚ö†Ô∏è Module d'analyse hybride non disponible - mode simple uniquement")
    HYBRID_AVAILABLE = False

# Module de gestion des prompts
try:
    from prompt_manager import prompt_manager, get_all_prompts_for_dropdown, get_prompt_content
    PROMPT_MANAGER_AVAILABLE = True
    print("‚úÖ Gestionnaire de prompts disponible")
except ImportError:
    print("‚ö†Ô∏è Gestionnaire de prompts non disponible")
    PROMPT_MANAGER_AVAILABLE = False

# ========================================
# CONFIGURATION GLOBALE
# ========================================

app_state = {
    "ollama_url": "http://localhost:11434",
    "models_list": ["mistral:7b", "mixtral:8x7b", "llama3.1:8b"],
    "chunk_analyzer": None,
    "hybrid_analyzer": None
}

# Prompts juridiques optimis√©s professionnels
PROMPTS_FALLBACK = {
    "Analyse juridique approfondie": """Tu es avocat sp√©cialis√© en droit du travail avec 15 ans d'exp√©rience au Barreau de Paris.

MISSION : Analyse juridique approfondie selon la m√©thodologie du Conseil d'√âtat :

1. QUALIFICATION JURIDIQUE DES FAITS
- Identifie la nature exacte des faits (licenciement, rupture conventionnelle, etc.)
- D√©termine les dispositions l√©gales applicables (Code du travail, Convention collective)
- Pr√©cise le r√©gime juridique et les conditions d'application

2. ANALYSE DES MOYENS DE DROIT
- Examine chaque argument juridique invoqu√© par les parties
- V√©rifie la conformit√© aux proc√©dures l√©gales obligatoires
- Identifie les vices de proc√©dure √©ventuels et leurs cons√©quences
- Analyse l'articulation des moyens principaux et subsidiaires

3. √âVALUATION DES PR√âTENTIONS
- Examine le bien-fond√© juridique de chaque demande
- Calcule les indemnit√©s selon les bar√®mes l√©gaux en vigueur
- √âvalue les chances de succ√®s devant la juridiction comp√©tente
- Identifie les risques et opportunit√©s proc√©durales

STYLE : Fran√ßais juridique pr√©cis, r√©daction narrative structur√©e SANS listes √† puces.
M√âTHODE : Raisonnement juridique rigoureux, citations exactes, pas de reformulation des faits.
FOCUS : Qualification juridique, moyens de droit, √©valuation des pr√©tentions.""",

    "Extraction par chunks": """Tu es juriste senior sp√©cialis√© en proc√©dure civile et droit du travail.

MISSION : Extraction juridique pr√©cise de ce fragment de document.

M√âTHODE D'ANALYSE :
- Identifie UNIQUEMENT les √©l√©ments juridiques pr√©sents dans ce passage
- Rel√®ve les moyens de droit invoqu√©s avec leur qualification exacte
- Note les r√©f√©rences l√©gales cit√©es (articles, jurisprudences, conventions)
- Extrait les demandes formul√©es avec leurs fondements

CONSIGNES STRICTES :
- Reste dans les limites de ce fragment, sans extrapolation
- Utilise le vocabulaire juridique fran√ßais appropri√©
- N'invente aucune information absente du texte
- Structure l'extraction par cat√©gories juridiques

STYLE : Concision juridique, terminologie pr√©cise, objectivit√© absolue.""",

    "Fusion juridique intelligente": """Tu es magistrat exp√©riment√© sp√©cialis√© en droit social.

MISSION : Fusion coh√©rente d'analyses juridiques partielles en synth√®se unifi√©e.

M√âTHODE DE SYNTH√àSE :
- Consolide les informations juridiques sans r√©p√©titions
- Respecte la chronologie proc√©durale et la logique juridique fran√ßaise
- Harmonise les qualifications juridiques et les r√©f√©rences l√©gales
- Structure selon l'ordre logique : qualification ‚Üí moyens ‚Üí pr√©tentions

OBJECTIFS :
- Cr√©er une analyse globale coh√©rente et structur√©e
- √âliminer les redondances tout en pr√©servant l'exhaustivit√©
- Maintenir la rigueur juridique et la pr√©cision terminologique
- Proposer une √©valuation √©quilibr√©e des positions des parties

STYLE : R√©daction juridique narrative, fran√ßais soutenu, argumentation structur√©e.""",

    "Synth√®se premium": """Tu es avocat aux Conseils et juriste d'entreprise senior.

MISSION : Synth√®se juridique premium de qualit√© Cour de cassation.

EXIGENCES DE QUALIT√â MAXIMALE :
- Analyse juridique de niveau expertise (15+ ans d'exp√©rience)
- R√©daction juridique impeccable selon les standards du Barreau
- Structure argumentative rigoureuse et logique implacable
- Qualification juridique pr√©cise et r√©f√©rences exactes

ARCHITECTURE DE LA SYNTH√àSE :
- Introduction : Qualification des faits et enjeux juridiques
- D√©veloppement : Analyse des moyens articul√©s selon leur force
- √âvaluation : Chances de succ√®s, risques, recommandations strat√©giques
- Conclusion : Position juridique synth√©tique et orientations

STYLE : Excellence r√©dactionnelle, terminologie juridique irr√©prochable, 
argumentation de niveau Conseil d'√âtat, clart√© p√©dagogique."""
}

# Types d'analyse disponibles
ANALYSIS_MODES = ["Simple directe", "Hybride par chunks"]

# ========================================
# FONCTIONS D'ANALYSE INT√âGR√âES
# ========================================

def analyze_simple_direct(text, prompt, model, provider, temperature, max_tokens, 
                         ollama_url, runpod_endpoint, runpod_token):
    """Analyse simple directe optimis√©e pour le juridique."""
    try:
        # Configuration optimis√©e pour analyses juridiques
        juridical_config = {
            "temperature": min(temperature, 0.4),  # Plus d√©terministe pour juridique
            "top_p": 0.8,                         # R√©duction cr√©ativit√©
            "repeat_penalty": 1.1,                # √âviter r√©p√©titions
            "num_ctx": 16384,                     # Contexte √©largi pour juridique
            "num_predict": max_tokens,
            "top_k": 40                           # Vocabulaire focalis√©
        }
        
        if provider == "RunPod.io":
            if not runpod_endpoint or not runpod_token:
                return "‚õî Endpoint et token RunPod requis"
            
            result = generate_with_runpod(
                model=model, 
                system_prompt=prompt, 
                user_text=text,
                num_ctx=juridical_config["num_ctx"],
                num_predict=juridical_config["num_predict"],
                temperature=juridical_config["temperature"],
                endpoint=runpod_endpoint,
                token=runpod_token
            )
        else:
            url = ollama_url if provider == "Ollama distant" else "http://localhost:11434"
            result = generate_with_ollama(
                model=model,
                system_prompt=prompt,
                user_text=text,
                num_ctx=juridical_config["num_ctx"],
                num_predict=juridical_config["num_predict"],
                temperature=juridical_config["temperature"],
                ollama_url=url
            )
        
        return result
    except Exception as e:
        return f"‚õî Erreur analyse simple: {str(e)}"

def analyze_hybrid_chunks(text, prompt, model, provider, temperature, max_tokens,
                         chunk_size, chunk_overlap, models_config,
                         ollama_url, runpod_endpoint, runpod_token):
    """Analyse hybride par chunks optimis√©e pour le juridique - utilise ChunkAnalyzer."""
    
    # Configuration chunks adapt√©e au juridique
    juridical_chunk_size = max(chunk_size, 4000)    # Chunks plus larges pour contexte juridique
    juridical_overlap = max(chunk_overlap, 400)     # Chevauchement important pour continuit√©
    
    # Utiliser prioritairement ChunkAnalyzer qui fonctionne
    if CHUNK_ANALYSIS_AVAILABLE:
        return analyze_by_chunks_enhanced(text, prompt, model, provider, temperature, max_tokens,
                                        juridical_chunk_size, juridical_overlap, ollama_url, runpod_endpoint, runpod_token)
    
    # Si module hybride disponible, essayer (mode exp√©rimental)
    elif HYBRID_AVAILABLE:
        try:
            return analyze_hybrid_mode_experimental(text, prompt, models_config, provider, temperature, max_tokens,
                                                  juridical_chunk_size, juridical_overlap, ollama_url, runpod_endpoint, runpod_token)
        except Exception as e:
            # Fallback vers chunks si hybride √©choue
            print(f"Hybride √©chou√©, fallback vers chunks: {e}")
            if CHUNK_ANALYSIS_AVAILABLE:
                return analyze_by_chunks_enhanced(text, prompt, model, provider, temperature, max_tokens,
                                                juridical_chunk_size, juridical_overlap, ollama_url, runpod_endpoint, runpod_token)
    
    # Fallback : analyse simple avec message informatif
    result = analyze_simple_direct(text, prompt, model, provider, temperature, max_tokens,
                                 ollama_url, runpod_endpoint, runpod_token)
    
    return f"""‚ö†Ô∏è ANALYSE SIMPLIFI√âE (Modules chunks non disponibles)

{result}

üí° RECOMMANDATION : Pour l'analyse juridique optimale par chunks, installez :
- chunck_analysis.py (d√©coupage intelligent par sections juridiques)
- hybrid_analyzer.py (analyse hybride 3 √©tapes : extraction ‚Üí fusion ‚Üí synth√®se premium)"""

def analyze_hybrid_mode_experimental(text, prompt, models_config, provider, temperature, max_tokens,
                                   chunk_size, chunk_overlap, ollama_url, runpod_endpoint, runpod_token):
    """Mode hybride avec votre module HybridAnalyzer r√©el."""
    try:
        # Initialiser selon votre API r√©elle
        if not app_state["hybrid_analyzer"]:
            # Cr√©er un ai_provider_manager compatible
            class AIProviderManager:
                def __init__(self):
                    self.provider = provider
                    self.ollama_url = ollama_url
                    self.runpod_endpoint = runpod_endpoint
                    self.runpod_token = runpod_token
                
                def call_model(self, model, system_prompt, user_text, **kwargs):
                    if provider == "RunPod.io":
                        return generate_with_runpod(
                            model=model,
                            system_prompt=system_prompt,
                            user_text=user_text,
                            num_ctx=16384,
                            num_predict=max_tokens,
                            temperature=0.3,
                            endpoint=runpod_endpoint,
                            token=runpod_token
                        )
                    else:
                        url = ollama_url if provider == "Ollama distant" else "http://localhost:11434"
                        return generate_with_ollama(
                            model=model,
                            system_prompt=system_prompt,
                            user_text=user_text,
                            num_ctx=16384,
                            num_predict=max_tokens,
                            temperature=0.3,
                            ollama_url=url
                        )
            
            ai_provider = AIProviderManager()
            app_state["hybrid_analyzer"] = create_hybrid_analyzer(ai_provider)
        
        analyzer = app_state["hybrid_analyzer"]
        
        # Utiliser analyze_document avec les bons param√®tres
        result = analyzer.analyze_document(
            text=text,
            analysis_type="juridique",
            system_prompt=prompt,
            provider=provider,
            model=models_config.get("step2", "mixtral:8x7b"),
            temperature=0.3,
            max_tokens=max_tokens,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # Formatage du r√©sultat selon le type de retour
        if isinstance(result, dict):
            analysis_content = result.get('content', result.get('analysis', str(result)))
            processing_time = result.get('processing_time', 0)
            confidence = result.get('confidence', 'N/A')
            
            formatted = f"""‚ö° ANALYSE HYBRIDE JURIDIQUE

üéØ Configuration :
- Type d'analyse : juridique sp√©cialis√©e
- Provider : {provider}
- Mod√®le principal : {models_config.get("step2", "mixtral:8x7b")}
- Chunks : {chunk_size} caract√®res

üìä M√©tadonn√©es :
- Dur√©e : {processing_time:.1f}s
- Confiance : {confidence}

üìù ANALYSE JURIDIQUE :

{analysis_content}

üí° Analyse produite avec HybridAnalyzer optimis√© juridique."""
        else:
            formatted = f"""‚ö° ANALYSE HYBRIDE JURIDIQUE

üìù R√âSULTAT :

{result}

üí° Analyse produite avec votre module HybridAnalyzer."""
        
        return formatted
        
    except Exception as e:
        return f"‚õî Erreur module hybride: {str(e)}"

def analyze_by_chunks_enhanced(text, prompt, model, provider, temperature, max_tokens,
                              chunk_size, chunk_overlap, ollama_url, runpod_endpoint, runpod_token):
    """Analyse par chunks optimis√©e avec prompts juridiques sp√©cialis√©s."""
    try:
        # Initialiser l'analyseur avec configuration juridique
        if not app_state["chunk_analyzer"]:
            app_state["chunk_analyzer"] = ChunkAnalyzer(chunk_size=chunk_size, overlap=chunk_overlap)
        
        analyzer = app_state["chunk_analyzer"]
        
        # D√©couper avec pr√©servation structure juridique
        chunks = analyzer.smart_chunk_text(text, preserve_structure=True)
        print(f"üìÑ D√©coupage juridique en {len(chunks)} chunks")
        
        # Configuration optimis√©e pour juridique
        juridical_config = {
            "temperature": 0.3,     # D√©terminisme pour extraction juridique
            "num_ctx": 12288,       # Contexte adapt√© aux chunks
            "num_predict": 2000,    # Analyses de chunks d√©taill√©es
            "top_p": 0.8,
            "repeat_penalty": 1.1
        }
        
        # Prompt sp√©cialis√© extraction juridique
        extraction_prompt = PROMPTS_FALLBACK["Extraction par chunks"]
        
        # Fonction d'IA optimis√©e juridique
        def ai_function_juridical(text, prompt, **kwargs):
            if provider == "RunPod.io":
                return generate_with_runpod(
                    model=model,
                    system_prompt=prompt,
                    user_text=text,
                    num_ctx=juridical_config["num_ctx"],
                    num_predict=juridical_config["num_predict"],
                    temperature=juridical_config["temperature"],
                    endpoint=runpod_endpoint,
                    token=runpod_token
                )
            else:
                url = ollama_url if provider == "Ollama distant" else "http://localhost:11434"
                return generate_with_ollama(
                    model=model,
                    system_prompt=prompt,
                    user_text=text,
                    num_ctx=juridical_config["num_ctx"],
                    num_predict=juridical_config["num_predict"],
                    temperature=juridical_config["temperature"],
                    ollama_url=url
                )
        
        # Analyser les chunks avec prompt sp√©cialis√©
        analyzed_chunks = analyzer.analyze_chunks_with_ai(chunks, extraction_prompt, ai_function_juridical)
        
        # Synth√®se avec prompt juridique sp√©cialis√©
        fusion_prompt = PROMPTS_FALLBACK["Fusion juridique intelligente"]
        
        synthesis = analyzer.synthesize_analyses(analyzed_chunks, fusion_prompt, ai_function_juridical)
        
        return f"""üìä ANALYSE JURIDIQUE PAR CHUNKS OPTIMIS√âE

üîß Configuration utilis√©e :
- Chunks juridiques : {len(chunks)} segments de {chunk_size:,} caract√®res
- Chevauchement : {chunk_overlap} caract√®res pour continuit√© proc√©durale
- Mod√®le d'extraction : {model} (temp√©rature 0.3 pour pr√©cision)
- Prompt sp√©cialis√© : Extraction juridique cibl√©e + Fusion intelligente

üìù SYNTH√àSE JURIDIQUE :

{synthesis}"""
        
    except Exception as e:
        return f"‚õî Erreur analyse juridique par chunks: {str(e)}"

def analyze_hybrid_mode_enhanced(text, prompt, models_config, provider, temperature, max_tokens,
                                chunk_size, chunk_overlap, ollama_url, runpod_endpoint, runpod_token):
    """Analyse hybride 3 √©tapes optimis√©e pour qualit√© juridique maximale."""
    if not HYBRID_AVAILABLE:
        return "‚õî Module d'analyse hybride non disponible"
    
    try:
        # Initialiser l'analyseur hybride selon votre API
        if not app_state["hybrid_analyzer"]:
            # Cr√©er un provider manager basique pour votre API
            class BasicAIProviderManager:
                def __init__(self):
                    self.provider = provider
                    self.ollama_url = ollama_url
                    self.runpod_endpoint = runpod_endpoint
                    self.runpod_token = runpod_token
                
                def call_ai(self, model, system_prompt, user_text, **kwargs):
                    if provider == "RunPod.io":
                        return generate_with_runpod(
                            model=model,
                            system_prompt=system_prompt,
                            user_text=user_text,
                            num_ctx=16384,
                            num_predict=max_tokens,
                            temperature=0.3,
                            endpoint=runpod_endpoint,
                            token=runpod_token
                        )
                    else:
                        url = ollama_url if provider == "Ollama distant" else "http://localhost:11434"
                        return generate_with_ollama(
                            model=model,
                            system_prompt=system_prompt,
                            user_text=user_text,
                            num_ctx=16384,
                            num_predict=max_tokens,
                            temperature=0.3,
                            ollama_url=url
                        )
            
            ai_provider = BasicAIProviderManager()
            app_state["hybrid_analyzer"] = create_hybrid_analyzer(ai_provider)
        
        analyzer = app_state["hybrid_analyzer"]
        
        # Configuration mod√®les optimale pour juridique
        step1_model = models_config.get("step1", "mistral:7b")
        step2_model = models_config.get("step2", "mixtral:8x7b")  
        step3_model = models_config.get("step3", "mixtral:8x7b")
        
        # Analyse hybride avec votre API
        result = analyzer.analyze_hybrid(
            text=text,
            user_prompt=prompt,
            provider=provider,
            ollama_url=ollama_url,
            runpod_endpoint=runpod_endpoint,
            runpod_token=runpod_token,
            step1_model=step1_model,
            step2_model=step2_model,
            step3_model=step3_model,
            temperature=0.3,
            max_tokens=max_tokens,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        if result.get("success"):
            formatted = f"""‚ö° ANALYSE JURIDIQUE HYBRIDE PREMIUM

üéØ Architecture 3 √©tapes optimis√©e :
‚îî‚îÄ‚îÄ Phase 1 : Extraction juridique cibl√©e ({step1_model})
‚îî‚îÄ‚îÄ Phase 2 : Fusion contextuelle intelligente ({step2_model}) 
‚îî‚îÄ‚îÄ Phase 3 : Synth√®se premium niveau expertise ({step3_model})

üìä Performance :
- Chunks trait√©s : {result.get('metadata', {}).get('chunks_count', 'N/A')}
- Dur√©e totale : {result.get('metadata', {}).get('processing_time', 0):.1f}s
- Qualit√© juridique : Premium (configuration optimis√©e)

üìù SYNTH√àSE JURIDIQUE FINALE :

{result.get('result', 'R√©sultat non disponible')}

üí° Analyse produite avec m√©thodologie d'expertise juridique fran√ßaise."""
            
            return formatted
        else:
            return f"‚õî Erreur analyse hybride: {result.get('error', 'Erreur inconnue')}"
            
    except Exception as e:
        return f"‚õî Erreur analyse hybride: {str(e)}"

def analyze_hybrid_mode(text, prompt, models_config, provider, temperature, max_tokens,
                       chunk_size, chunk_overlap, ollama_url, runpod_endpoint, runpod_token):
    """Analyse hybride 3 √©tapes (si module disponible)."""
    if not HYBRID_AVAILABLE:
        return "‚õî Module d'analyse hybride non disponible - utilisez l'analyse par chunks"
    
    try:
        # Initialiser l'analyseur hybride
        if not app_state["hybrid_analyzer"]:
            app_state["hybrid_analyzer"] = create_hybrid_analyzer(
                chunk_size=chunk_size,
                overlap=chunk_overlap
            )
        
        analyzer = app_state["hybrid_analyzer"]
        
        # Configuration des mod√®les par √©tape
        step1_model = models_config.get("step1", model)
        step2_model = models_config.get("step2", model)  
        step3_model = models_config.get("step3", model)
        
        # Analyse hybride
        result = analyzer.analyze_hybrid(
            text=text,
            user_prompt=prompt,
            provider=provider,
            ollama_url=ollama_url,
            runpod_endpoint=runpod_endpoint,
            runpod_token=runpod_token,
            step1_model=step1_model,
            step2_model=step2_model,
            step3_model=step3_model,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        if result["success"]:
            return analyzer.format_hybrid_result(result, prompt, f"{len(text):,} caract√®res")
        else:
            return f"‚õî Erreur analyse hybride: {result['error']}"
            
    except Exception as e:
        return f"‚õî Erreur analyse hybride: {str(e)}"

def analyze_text_complete(text, prompt, analysis_mode, model, provider, temperature, max_tokens,
                         chunk_size, chunk_overlap, models_config,
                         ollama_url, runpod_endpoint, runpod_token):
    """Fonction d'analyse compl√®te qui route vers le bon mode."""
    if not text.strip():
        return "‚õî Aucun texte √† analyser", "", "", "Aucun texte"
    
    start_time = time.time()
    
    try:
        print(f"üöÄ Analyse {analysis_mode} - Mod√®le: {model} - Provider: {provider}")
        
        # Router vers le bon mode d'analyse
        if analysis_mode == "Simple directe":
            result = analyze_simple_direct(
                text, prompt, model, provider, temperature, max_tokens,
                ollama_url, runpod_endpoint, runpod_token
            )
        
        elif analysis_mode == "Hybride par chunks":
            result = analyze_hybrid_chunks(
                text, prompt, model, provider, temperature, max_tokens,
                chunk_size, chunk_overlap, models_config,
                ollama_url, runpod_endpoint, runpod_token
            )
        
        else:
            result = "‚õî Mode d'analyse non reconnu"
        
        # Formatage du r√©sultat
        processing_time = time.time() - start_time
        timestamp = datetime.now().strftime("%d/%m/%Y √† %H:%M:%S")
        
        formatted_result = f"""{'=' * 80}
ANALYSE JURIDIQUE COMPL√àTE - {timestamp}
{'=' * 80}

MODE: {analysis_mode}
MOD√àLE: {model}
FOURNISSEUR: {provider}
TEXTE: {len(text):,} caract√®res
DUR√âE: {processing_time:.1f} secondes

{'-' * 80}
PROMPT UTILIS√â
{'-' * 80}

{prompt}

{'-' * 80}
R√âSULTAT DE L'ANALYSE
{'-' * 80}

{result}
"""
        
        stats = f"{len(text):,} caract√®res"
        debug = f"Analyse {analysis_mode} - {provider} - {model} - {processing_time:.1f}s"
        
        return formatted_result, stats, debug, f"Analyse {analysis_mode} termin√©e"
        
    except Exception as e:
        error_msg = f"‚õî ERREUR ANALYSE: {str(e)}"
        return error_msg, "", error_msg, "Erreur"

# ========================================
# FONCTIONS DE CONFIGURATION
# ========================================

def test_connection_complete(provider, ollama_url, runpod_endpoint, runpod_token):
    """Test de connexion utilisant les modules existants."""
    try:
        if hasattr(test_ai_connection, '__call__'):  # Si ai_wrapper disponible
            success, models = test_ai_connection(provider, ollama_url, runpod_endpoint, runpod_token)
            return success, models
        else:
            # Fallback sur les fonctions simples
            if provider == "RunPod.io":
                if not runpod_endpoint or not runpod_token:
                    return "‚õî Endpoint et token requis", []
                headers = {"Authorization": f"Bearer {runpod_token}", "Content-Type": "application/json"}
                response = requests.get(f"{runpod_endpoint}/v1/models", headers=headers, timeout=10)
                if response.status_code in [200, 404]:
                    models = ["meta-llama/Llama-3.1-70B-Instruct", "mistralai/Mistral-7B-Instruct-v0.3"]
                    return "‚úÖ Connexion RunPod r√©ussie", models
                else:
                    return f"‚õî Erreur RunPod {response.status_code}", []
            else:
                url = ollama_url if provider == "Ollama distant" else "http://localhost:11434"
                models = get_ollama_models(url)
                return f"‚úÖ Connexion Ollama - {len(models)} mod√®les", models
                
    except Exception as e:
        return f"‚õî Erreur test connexion: {str(e)}", []

def get_available_prompts():
    """R√©cup√®re tous les prompts disponibles."""
    if PROMPT_MANAGER_AVAILABLE:
        try:
            return get_all_prompts_for_dropdown()
        except:
            pass
    return list(PROMPTS_FALLBACK.keys())

def load_prompt_content(prompt_name):
    """Charge le contenu d'un prompt."""
    if PROMPT_MANAGER_AVAILABLE:
        try:
            content = get_prompt_content(prompt_name)
            if content:
                return content
        except:
            pass
    return PROMPTS_FALLBACK.get(prompt_name, "")

# ========================================
# INTERFACE GRADIO COMPL√àTE
# ========================================

def create_complete_interface():
    """Interface Gradio compl√®te avec tous les modes d'analyse."""
    
    with gr.Blocks(title="OCR Juridique - Version Compl√®te", theme=gr.themes.Soft()) as demo:
        
        gr.Markdown(f"""
        # üìö OCR Juridique - Version Compl√®te v8.9
        
        **Modes d'analyse disponibles :**
        - üöÄ **Simple directe** : Analyse en une fois (rapide)
        - ‚ö° **Hybride par chunks** : D√©coupage intelligent ‚Üí Extraction ‚Üí Fusion ‚Üí Synth√®se {('‚úÖ' if HYBRID_AVAILABLE or CHUNK_ANALYSIS_AVAILABLE else '‚õî')}
        
        **Modules charg√©s :**
        - Traitement fichiers : ‚úÖ
        - Analyse par chunks/hybride : {('‚úÖ' if CHUNK_ANALYSIS_AVAILABLE or HYBRID_AVAILABLE else '‚õî')}
        - Gestionnaire prompts : {('‚úÖ' if PROMPT_MANAGER_AVAILABLE else '‚õî')}
        """)
        
        with gr.Tabs():
            
            # ===== CONFIGURATION =====
            with gr.Tab("‚öôÔ∏è Configuration"):
                with gr.Row():
                    with gr.Column():
                        provider = gr.Radio(
                            choices=["Ollama local", "Ollama distant", "RunPod.io"],
                            value="Ollama local",
                            label="Fournisseur IA"
                        )
                        
                        ollama_url = gr.Textbox(
                            label="URL Ollama distant",
                            value=app_state["ollama_url"],
                            visible=False
                        )
                        
                        runpod_endpoint = gr.Textbox(
                            label="Endpoint RunPod",
                            visible=False
                        )
                        
                        runpod_token = gr.Textbox(
                            label="Token RunPod",
                            type="password",
                            visible=False
                        )
                        
                        test_btn = gr.Button("üîç Tester connexion", variant="primary")
                        status = gr.Textbox(label="Statut", interactive=False, value="Pr√™t")
                    
                    with gr.Column():
                        gr.Markdown("### Mod√®les")
                        
                        model_main = gr.Dropdown(
                            choices=app_state["models_list"],
                            value="mistral:7b",
                            label="Mod√®le principal"
                        )
                        
                        # Configuration mod√®les optimis√©e juridique
                        with gr.Group(visible=(HYBRID_AVAILABLE or CHUNK_ANALYSIS_AVAILABLE)) as hybrid_models_group:
                            gr.Markdown("#### üèõÔ∏è Configuration expertise juridique par phase")
                            
                            model_step1 = gr.Dropdown(
                                choices=app_state["models_list"],
                                value="mistral:7b",
                                label="‚ö° Phase 1 - Extraction juridique",
                                info="Mod√®le rapide pour identifier moyens de droit (Mistral 7B optimal)"
                            )
                            model_step2 = gr.Dropdown(
                                choices=app_state["models_list"], 
                                value="mixtral:8x7b" if "mixtral:8x7b" in app_state["models_list"] else app_state["models_list"][0],
                                label="üß† Phase 2 - Fusion proc√©durale",
                                info="Raisonnement juridique complexe (Mixtral 8x7B recommand√©)"
                            )
                            model_step3 = gr.Dropdown(
                                choices=app_state["models_list"],
                                value="mixtral:8x7b" if "mixtral:8x7b" in app_state["models_list"] else app_state["models_list"][0],
                                label="‚öñÔ∏è Phase 3 - Synth√®se d'expertise",
                                info="Qualit√© r√©dactionnelle niveau Barreau (Mixtral 8x7B optimal)"
                            )
                            
                            # Param√®tres juridiques optimis√©s
                            gr.Markdown("##### ‚öôÔ∏è Param√®tres juridiques optimis√©s")
                            
                            with gr.Row():
                                chunk_size = gr.Slider(
                                    3000, 8000, value=5000, step=500,
                                    label="Taille chunks juridiques",
                                    info="Chunks plus larges pour contexte proc√©dural"
                                )
                                chunk_overlap = gr.Slider(
                                    200, 800, value=500, step=100,
                                    label="Chevauchement proc√©dural", 
                                    info="Continuit√© entre sections juridiques"
                                )
                        
                        with gr.Row():
                            temperature = gr.Slider(
                                0, 1, value=0.3, step=0.1, 
                                label="Temp√©rature juridique",
                                info="0.2-0.4 optimal pour analyses juridiques (d√©terminisme)"
                            )
                            max_tokens = gr.Slider(
                                1000, 6000, value=4000, step=500, 
                                label="Longueur analyse",
                                info="Analyses juridiques d√©taill√©es (3000-5000 recommand√©)"
                            )
            
            # ===== DOCUMENTS =====
            with gr.Tab("üìÑ Documents"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### Document")
                        file_input = gr.File(label="Fichier", file_types=[".pdf", ".txt", ".docx"])
                        
                        with gr.Row():
                            clean_text = gr.Checkbox(label="Nettoyer", value=True)
                            anonymize = gr.Checkbox(label="Anonymiser", value=False)
                        
                        text_input = gr.Textbox(label="Texte", lines=15)
                        stats = gr.Textbox(label="Statistiques", interactive=False)
                    
                    with gr.Column():
                        gr.Markdown("### Configuration d'analyse")
                        
                        analysis_mode = gr.Radio(
                            choices=[mode for mode in ANALYSIS_MODES if 
                                   mode == "Simple directe" or 
                                   (mode == "Hybride par chunks" and (CHUNK_ANALYSIS_AVAILABLE or HYBRID_AVAILABLE))],
                            value="Simple directe",
                            label="Mode d'analyse"
                        )
                        
                        # Configuration chunks adapt√©e juridique
                        with gr.Group() as chunk_config:
                            gr.Markdown("##### üìÑ D√©coupage proc√©dural intelligent")
                            
                            with gr.Row():
                                chunk_size = gr.Slider(
                                    3000, 8000, value=5000, step=500,
                                    label="Taille segments juridiques",
                                    info="Segments plus larges pour contexte proc√©dural complet"
                                )
                                chunk_overlap = gr.Slider(
                                    200, 800, value=500, step=100,
                                    label="Continuit√© proc√©durale",
                                    info="Chevauchement pour pr√©server liens juridiques"
                                )
                        
                        # S√©lection de prompts juridiques
                        prompt_selector = gr.Dropdown(
                            choices=get_available_prompts(),
                            value=get_available_prompts()[0] if get_available_prompts() else "",
                            label="üìã Expertise juridique pr√©d√©finie",
                            info="Prompts optimis√©s par sp√©cialit√© juridique"
                        )
                        
                        load_prompt_btn = gr.Button("üìù Charger prompt")
            
            # ===== ANALYSE =====
            with gr.Tab("üîç Analyse"):
                prompt_text = gr.Textbox(
                    label="Prompt d'analyse",
                    lines=12,
                    value=load_prompt_content(get_available_prompts()[0]) if get_available_prompts() else "",
                    placeholder="D√©crivez l'analyse juridique souhait√©e..."
                )
                
                analyze_btn = gr.Button("üöÄ Lancer l'analyse", variant="primary", size="lg")
            
            # ===== R√âSULTATS =====
            with gr.Tab("üìä R√©sultats"):
                result_text = gr.Textbox(
                    label="R√©sultat de l'analyse",
                    lines=30,
                    show_copy_button=True
                )
                
                with gr.Row():
                    clear_btn = gr.Button("üóëÔ∏è Nettoyer", variant="stop")
                    
                    with gr.Column():
                        debug_info = gr.Textbox(
                            label="Informations de debug",
                            lines=5,
                            interactive=False
                        )
        
        # ========================================
        # √âV√âNEMENTS
        # ========================================
        
        # Configuration provider
        def on_provider_change(prov):
            return (
                gr.update(visible=(prov == "Ollama distant")),
                gr.update(visible=(prov == "RunPod.io")),
                gr.update(visible=(prov == "RunPod.io")),
                f"Provider: {prov}"
            )
        
        provider.change(
            on_provider_change,
            inputs=[provider],
            outputs=[ollama_url, runpod_endpoint, runpod_token, status]
        )
        
        # Test connexion et mise √† jour des mod√®les avec s√©lection intelligente
        def test_and_update_models(prov, url, endpoint, token):
            message, models = test_connection_complete(prov, url, endpoint, token)
            
            if models:
                app_state["models_list"] = models
                
                # S√©lection intelligente des mod√®les par d√©faut
                def smart_select_model(preferred_models, fallback_index=0):
                    for preferred in preferred_models:
                        if preferred in models:
                            return preferred
                    return models[fallback_index] if len(models) > fallback_index else models[0]
                
                # Mod√®les optimaux par phase
                step1_model = smart_select_model(["mistral:7b", "mistral:latest", "llama3.1:8b"], 0)
                step2_model = smart_select_model(["mixtral:8x7b", "llama3.1:8b", "mistral:latest"], 
                                               min(1, len(models)-1))
                step3_model = smart_select_model(["llama3.1:8b", "mixtral:8x7b", "mistral:latest"], 
                                               min(2, len(models)-1))
                
                return (
                    gr.update(choices=models, value=models[0]),  # model_main
                    gr.update(choices=models, value=step1_model),  # model_step1
                    gr.update(choices=models, value=step2_model),  # model_step2  
                    gr.update(choices=models, value=step3_model),  # model_step3
                    f"‚úÖ {message} | Mod√®les optimis√©s: {step1_model} ‚Üí {step2_model} ‚Üí {step3_model}"
                )
            else:
                return (
                    gr.update(), gr.update(), gr.update(), gr.update(),
                    f"‚õî {message}"
                )
        
        test_btn.click(
            test_and_update_models,
            inputs=[provider, ollama_url, runpod_endpoint, runpod_token],
            outputs=[model_main, model_step1, model_step2, model_step3, status]
        )
        
        # Gestion fichiers
        def handle_file_upload(file, clean, anon):
            if file:
                try:
                    message, stats_result, text, file_type, anon_report = process_file_to_text(
                        file.name, clean, anon, force_ocr=False
                    )
                    return message, stats_result, text
                except Exception as e:
                    return f"‚õî Erreur: {str(e)}", "Erreur", ""
            return "", "0 caract√®res", ""
        
        file_input.change(
            handle_file_upload,
            inputs=[file_input, clean_text, anonymize],
            outputs=[status, stats, text_input]
        )
        
        # Chargement de prompts
        def load_selected_prompt(prompt_name):
            content = load_prompt_content(prompt_name)
            return content, f"‚úÖ Prompt '{prompt_name}' charg√©"
        
        load_prompt_btn.click(
            load_selected_prompt,
            inputs=[prompt_selector],
            outputs=[prompt_text, status]
        )
        
        # Affichage conditionnel de la config hybride selon le mode
        def toggle_hybrid_config(mode):
            return gr.update(visible=(mode == "Hybride par chunks"))
        
        analysis_mode.change(
            toggle_hybrid_config,
            inputs=[analysis_mode],
            outputs=[hybrid_models_group]
        )
        
        # Analyse principale
        def run_complete_analysis(text, prompt, mode, model_main, provider, temp, max_tok,
                                chunk_sz, chunk_ovlp, model1, model2, model3,
                                ollama_url, runpod_endpoint, runpod_token):
            
            models_config = {
                "step1": model1 if HYBRID_AVAILABLE else model_main,
                "step2": model2 if HYBRID_AVAILABLE else model_main,
                "step3": model3 if HYBRID_AVAILABLE else model_main
            }
            
            return analyze_text_complete(
                text, prompt, mode, model_main, provider, temp, max_tok,
                chunk_sz, chunk_ovlp, models_config,
                ollama_url, runpod_endpoint, runpod_token
            )
        
        analyze_btn.click(
            run_complete_analysis,
            inputs=[
                text_input, prompt_text, analysis_mode, model_main, provider, 
                temperature, max_tokens, chunk_size, chunk_overlap
            ] + [model_step1, model_step2, model_step3] + [
                ollama_url, runpod_endpoint, runpod_token
            ],
            outputs=[result_text, stats, debug_info, status]
        )
        
        # Nettoyage
        def clear_all_fields():
            return "", "", "", "üßπ Interface nettoy√©e", ""
        
        clear_btn.click(
            clear_all_fields,
            outputs=[text_input, result_text, stats, status, debug_info]
        )
    
    return demo

# ========================================
# FONCTIONS D'EXPORT
# ========================================

def build_ui():
    """Point d'entr√©e pour main_ocr.py"""
    print("üöÄ Construction interface compl√®te avec analyse hybride par chunks...")
    print(f"‚ö° Analyse hybride/chunks: {'‚úÖ Disponible' if (CHUNK_ANALYSIS_AVAILABLE or HYBRID_AVAILABLE) else '‚õî Non disponible'}")
    print(f"üìù Gestionnaire prompts: {'‚úÖ Disponible' if PROMPT_MANAGER_AVAILABLE else '‚õî Non disponible'}")
    return create_complete_interface()

# ========================================
# LANCEMENT DIRECT
# ========================================

if __name__ == "__main__":
    print("üß™ Test interface compl√®te")
    demo = create_complete_interface()
    
    ports_to_try = [7860, 7861, 7862, 7863, 7864]
    
    for port in ports_to_try:
        try:
            print(f"üåê Lancement sur port {port}")
            demo.launch(
                server_port=port,
                server_name="127.0.0.1",
                share=False,
                show_error=True
            )
            break
        except Exception as e:
            print(f"‚õî Port {port} occup√©: {e}")
            continue
