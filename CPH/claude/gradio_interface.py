#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Interface utilisateur Gradio pour OCR Juridique - VERSION AVEC .ENV
Version: 8.4-ENV-COMPLETE
Date: 2025-09-16
Utilise VOS modules existants + gestion s√©curis√©e des cl√©s API via .env
"""

import os
import gradio as gr
from datetime import datetime
import json
import threading
import time
import requests


def get_optimal_gpu_config(model_name: str, text_length: int) -> dict:
    """Retourne une configuration GPU optimale selon le mod√®le et la taille du texte."""
    
    # Configuration de base pour forcer le GPU
    base_config = {
        "num_gpu": 50,  # Forcer l'utilisation GPU
        "num_batch": 512,
        "num_thread": 8,
        "repeat_penalty": 1.1,
        "top_p": 0.9
    }
    
    # Ajustements selon le mod√®le
    if "7b" in model_name.lower():
        base_config.update({
            "num_gpu": 50,  # Utilisation GPU maximale
            "num_batch": 512,
            "num_ctx": 8192 if text_length > 4000 else 4096
        })
    elif "13b" in model_name.lower():
        base_config.update({
            "num_gpu": 45,  # L√©g√®rement moins pour mod√®les plus gros
            "num_batch": 256,
            "num_ctx": 4096
        })
    elif "mixtral" in model_name.lower() or "8x7b" in model_name.lower():
        base_config.update({
            "num_gpu": 40,  # Mixtral n√©cessite plus de VRAM
            "num_batch": 256,
            "num_ctx": 8192
        })
    elif "70b" in model_name.lower():
        base_config.update({
            "num_gpu": 30,  # Gros mod√®les
            "num_batch": 128,
            "num_ctx": 4096
        })
    
    # Ajustements selon la taille du texte
    if text_length > 8000:
        base_config["num_ctx"] = min(base_config.get("num_ctx", 4096), 8192)
        base_config["num_batch"] = max(base_config.get("num_batch", 256), 512)
    
    return base_config

# ========================================
# VARIABLES GLOBALES - INITIALISATION
# ========================================

# Initialiser les variables globalement pour √©viter les erreurs
MODULES_AVAILABLE = False
ENV_AVAILABLE = False

# ========================================
# IMPORTS DES MODULES EXISTANTS + ENV
# ========================================

try:
    # VOS modules qui existent r√©ellement
    from ai_providers import (
        generate_with_ollama, 
        generate_with_runpod, 
        get_ollama_models,
        test_connection,
        calculate_smart_timeout
    )
    from chunck_analysis import ChunkAnalyzer
    from prompt_manager import prompt_manager, get_all_prompts_for_dropdown, get_prompt_content
    from processing_pipeline import process_file_to_text, do_analysis_only
    from config import DEFAULT_PROMPT_TEXT, EXPERT_PROMPT_TEXT
    
    MODULES_AVAILABLE = True
    print("‚úÖ Tous les modules existants charg√©s avec succ√®s")
    
except ImportError as e:
    print(f"‚ùå Erreur import modules: {e}")
    MODULES_AVAILABLE = False

# Gestionnaire d'environnement (nouveau)
try:
    from env_manager import (
        env_manager, 
        get_api_config, 
        get_app_config, 
        get_chunk_config,
        create_env_management_interface
    )
    ENV_AVAILABLE = True
    print("‚úÖ Gestionnaire .env charg√©")
except ImportError:
    ENV_AVAILABLE = False
    print("‚ö†Ô∏è Gestionnaire .env non disponible - continuez avec interface actuelle")

# ========================================
# CONFIGURATION R√âELLE BAS√âE SUR VOS MODULES + .ENV
# ========================================

# Domaines support√©s (bas√©s sur vos prompts existants)
SUPPORTED_DOMAINS = {
    "droit_travail": "Droit du travail",
    "contractuel": "Droit contractuel", 
    "procedure": "Proc√©dure civile",
    "immobilier": "Droit immobilier"
}

# Types de synth√®se disponibles
SYNTHESIS_TYPES = {
    "synthese_executive": "Synth√®se ex√©cutive",
    "analyse_detaillee": "Analyse d√©taill√©e",
    "rapport_structure": "Rapport structur√©",
    "conclusions": "Conclusions juridiques"
}

# Configuration depuis .env si disponible, sinon par d√©faut
if ENV_AVAILABLE and MODULES_AVAILABLE:
    try:
        api_config = get_api_config()
        chunk_config = get_chunk_config()
        DEFAULT_CONFIG = {
            "provider": "Ollama local",
            "chunk_size": chunk_config["chunk_size"],
            "chunk_overlap": chunk_config["chunk_overlap"],
            "ollama_url": api_config["ollama_url"]
        }
        print(f"üîë Configuration charg√©e depuis .env")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur .env: {e}, utilisation config par d√©faut")
        DEFAULT_CONFIG = {
            "provider": "Ollama local",
            "chunk_size": 3000,
            "chunk_overlap": 200,
            "ollama_url": "http://localhost:11434"
        }
else:
    DEFAULT_CONFIG = {
        "provider": "Ollama local",
        "chunk_size": 3000,
        "chunk_overlap": 200,
        "ollama_url": "http://localhost:11434"
    }

app_state = {
    "current_provider": "Ollama local",
    "ollama_url": "http://localhost:11434",
    "models_list": ["mistral:7b-instruct", "llama3:latest", "mixtral:8x7b"],
    "chunk_analyzer": None,
    "ready": MODULES_AVAILABLE
}

# ========================================
# GESTION ARR√äT AUTOMATIQUE RUNPOD
# ========================================

class RunPodManager:
    """Gestionnaire pour l'arr√™t automatique des pods RunPod."""
    
    def __init__(self):
        self.last_activity = time.time()
        self.auto_stop_enabled = False
        self.timeout_minutes = 15
        self.pod_id = None
        self.api_key = None
        self.monitor_thread = None
        
    def update_activity(self):
        """Met √† jour le timestamp de derni√®re activit√©."""
        self.last_activity = time.time()
        
    def configure_auto_stop(self, endpoint, token, timeout_minutes=15):
        """Configure l'arr√™t automatique."""
        try:
            if "runpod" in endpoint and token:
                self.pod_id = self._extract_pod_id(endpoint)
                self.api_key = token
                self.timeout_minutes = timeout_minutes
                self.auto_stop_enabled = True
                
                if not self.monitor_thread or not self.monitor_thread.is_alive():
                    self.monitor_thread = threading.Thread(target=self._monitor_activity, daemon=True)
                    self.monitor_thread.start()
                
                return f"Arr√™t auto configur√©: {timeout_minutes}min d'inactivit√©"
            else:
                return "Configuration arr√™t auto √©chou√©e"
        except Exception as e:
            return f"Erreur config arr√™t auto: {str(e)}"
    
    def _extract_pod_id(self, endpoint):
        """Extrait l'ID du pod depuis l'endpoint."""
        try:
            if "runpod" in endpoint:
                parts = endpoint.split("//")[1].split(".")[0].split("-")
                return parts[-1] if len(parts) > 1 else None
        except:
            pass
        return None
    
    def _monitor_activity(self):
        """Thread de monitoring de l'activit√©."""
        while self.auto_stop_enabled:
            try:
                time.sleep(60)
                
                if self.auto_stop_enabled and self.pod_id and self.api_key:
                    inactive_time = (time.time() - self.last_activity) / 60
                    
                    if inactive_time > self.timeout_minutes:
                        print(f"Inactivit√© d√©tect√©e: {inactive_time:.1f}min")
                        self._stop_pod()
                        break
                        
            except Exception as e:
                print(f"Erreur monitoring RunPod: {e}")
                break
    
    def _stop_pod(self):
        """Arr√™te le pod RunPod."""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            url = f"https://api.runpod.ai/v2/{self.pod_id}/terminate"
            response = requests.post(url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                print(f"Pod {self.pod_id} arr√™t√© automatiquement")
                self.auto_stop_enabled = False
            else:
                print(f"√âchec arr√™t pod: {response.status_code}")
                
        except Exception as e:
            print(f"Erreur arr√™t pod: {e}")
    
    def disable_auto_stop(self):
        """D√©sactive l'arr√™t automatique."""
        self.auto_stop_enabled = False
        return "Arr√™t automatique d√©sactiv√©"

# Instance globale du gestionnaire
runpod_manager = RunPodManager()

# ========================================
# FONCTIONS UTILITAIRES ADAPT√âES √Ä VOS MODULES
# ========================================

def load_config():
    """Charge la configuration."""
    if MODULES_AVAILABLE:
        try:
            # Initialiser l'analyseur de chunks avec VOS param√®tres
            app_state["chunk_analyzer"] = ChunkAnalyzer(
                chunk_size=DEFAULT_CONFIG["chunk_size"],
                overlap=DEFAULT_CONFIG["chunk_overlap"]
            )
            return True
        except Exception as e:
            print(f"Erreur chargement config: {e}")
    return False

def track_activity():
    """Marque une activit√© utilisateur (pour RunPod)."""
    runpod_manager.update_activity()

# ========================================
# FONCTIONS D'ANALYSE ADAPT√âES √Ä VOS MODULES
# ========================================

def analyze_hybrid_mode(text1, text2, prompt, model_step1, model_step2, model_step3,
                        provider, temperature, top_p, max_tokens, 
                        chunk_size, chunk_overlap, domain, synthesis_type, enable_step3,
                        ollama_url, runpod_endpoint, runpod_token):
    """Analyse hybride en utilisant VOS modules existants."""
    
    track_activity()
    
    # Si .env disponible et param√®tres manquants, utiliser .env
    if ENV_AVAILABLE:
        try:
            config = get_api_config()
            if not ollama_url and provider == "Ollama distant":
                ollama_url = config.get("ollama_remote_url", ollama_url)
            if provider == "RunPod.io":
                if not runpod_endpoint:
                    runpod_endpoint = config.get("runpod_endpoint", "")
                if not runpod_token:
                    runpod_token = config.get("runpod_token", "")
        except:
            pass
    
    if not MODULES_AVAILABLE:
        return fallback_analysis(text1, text2, prompt)
    
    # Pr√©paration du texte
    if text1 and text2:
        full_text = f"=== DOCUMENT 1 ===\n{text1}\n\n=== DOCUMENT 2 ===\n{text2}"
        doc_info = f"2 documents - {len(text1):,} + {len(text2):,} caract√®res"
    elif text1:
        full_text = text1
        doc_info = f"1 document - {len(text1):,} caract√®res"
    elif text2:
        full_text = text2
        doc_info = f"1 document - {len(text2):,} caract√®res"
    else:
        return ("ERREUR: Aucun texte fourni", "", "", "Aucun texte", "")
    
    try:
        start_time = time.time()
        
        # √âTAPE 1: D√©coupage et extraction avec le mod√®le rapide
        print(f"üìÑ √âtape 1: D√©coupage et extraction avec {model_step1}")
        
        analyzer = ChunkAnalyzer(chunk_size=chunk_size, overlap=chunk_overlap)
        chunks = analyzer.smart_chunk_text(full_text, preserve_structure=True)
        
        # Analyse de chaque chunk avec le mod√®le rapide (√âtape 1)
        step1_results = []
        step1_start = time.time()
        
        for chunk in chunks:
            chunk_prompt = get_prompt_content("Prompt par chunk") or prompt
            
            if provider == "RunPod.io":
                result = generate_with_runpod(
                    model=model_step1,
                    system_prompt=chunk_prompt,
                    user_text=chunk['text'],
                    num_ctx=4096,
                    num_predict=1000,
                    temperature=temperature,
                    endpoint=runpod_endpoint,
                    token=runpod_token
                )
            else:
                result = generate_with_ollama(
                    model=model_step1,
                    system_prompt=chunk_prompt,
                    user_text=chunk['text'],
                    num_ctx=4096,
                    num_predict=1000,
                    temperature=temperature,
                    ollama_url=ollama_url
                )
            
            step1_results.append(result)
            time.sleep(0.5)  # Pause entre chunks
        
        step1_duration = time.time() - step1_start
        
        # √âTAPE 2: Fusion avec le mod√®le √† grand contexte
        print(f"üîÄ √âtape 2: Fusion avec {model_step2}")
        step2_start = time.time()
        
        # Combiner tous les r√©sultats de l'√©tape 1
        combined_analyses = "\n\n".join([f"=== ANALYSE CHUNK {i+1} ===\n{result}" 
                                        for i, result in enumerate(step1_results)])
        
        fusion_prompt = get_prompt_content("Prompt de fusion") or f"""
{prompt}

T√ÇCHE SP√âCIALE: Fusionnez les analyses suivantes en un rapport coh√©rent et structur√©.
√âliminez les redondances et cr√©ez une synth√®se unifi√©e.

ANALYSES √Ä FUSIONNER:
{combined_analyses}
"""
        
        if provider == "RunPod.io":
            step2_result = generate_with_runpod(
                model=model_step2,
                system_prompt=fusion_prompt,
                user_text="FUSION DES ANALYSES DEMAND√âE",
                num_ctx=8192,
                num_predict=max_tokens,
                temperature=temperature,
                endpoint=runpod_endpoint,
                token=runpod_token
            )
        else:
            step2_result = generate_with_ollama(
                model=model_step2,
                system_prompt=fusion_prompt,
                user_text="FUSION DES ANALYSES DEMAND√âE",
                num_ctx=8192,
                num_predict=max_tokens,
                temperature=temperature,
                ollama_url=ollama_url
            )
        
        step2_duration = time.time() - step2_start
        
        # √âTAPE 3: Synth√®se premium (optionnelle)
        step3_result = step2_result
        step3_duration = 0
        
        if enable_step3:
            print(f"‚ú® √âtape 3: Synth√®se premium avec {model_step3}")
            step3_start = time.time()
            
            synthesis_prompt = f"""
{prompt}

T√ÇCHE DE SYNTH√àSE PREMIUM: Cr√©ez une synth√®se narrative de haute qualit√© √† partir de l'analyse suivante.
Type de synth√®se demand√©: {synthesis_type}
Domaine sp√©cialis√©: {domain if domain != "Aucun" else "G√©n√©ral"}

Am√©liorez la qualit√© r√©dactionnelle, la structure et la clart√©.

ANALYSE √Ä AM√âLIORER:
{step2_result}
"""
            
            if provider == "RunPod.io":
                step3_result = generate_with_runpod(
                    model=model_step3,
                    system_prompt=synthesis_prompt,
                    user_text="SYNTH√àSE PREMIUM DEMAND√âE",
                    num_ctx=4096,
                    num_predict=max_tokens,
                    temperature=temperature + 0.1,  # L√©g√®rement plus cr√©atif
                    endpoint=runpod_endpoint,
                    token=runpod_token
                )
            else:
                step3_result = generate_with_ollama(
                    model=model_step3,
                    system_prompt=synthesis_prompt,
                    user_text="SYNTH√àSE PREMIUM DEMAND√âE",
                    num_ctx=4096,
                    num_predict=max_tokens,
                    temperature=temperature + 0.1,
                    ollama_url=ollama_url
                )
            
            step3_duration = time.time() - step3_start
        
        total_duration = time.time() - start_time
        
        # Formatage du r√©sultat final
        current_time = datetime.now().strftime("%d/%m/%Y √† %H:%M:%S")
        
        formatted_result = f"""{'=' * 80}
                    ANALYSE JURIDIQUE HYBRIDE 3 √âTAPES
{'=' * 80}

HORODATAGE: {current_time}
ARCHITECTURE: Extraction ‚Üí Fusion ‚Üí Synth√®se
{doc_info}
CONFIGURATION: {'Depuis .env' if ENV_AVAILABLE else 'Par d√©faut'}

MOD√àLES UTILIS√âS:
üöÄ √âtape 1 (Extraction): {model_step1} - {step1_duration:.1f}s
üîÄ √âtape 2 (Fusion): {model_step2} - {step2_duration:.1f}s
{'‚ú® √âtape 3 (Synth√®se): ' + model_step3 + f' - {step3_duration:.1f}s' if enable_step3 else '‚óã √âtape 3: D√©sactiv√©e'}

PERFORMANCE:
- Chunks trait√©s: {len(chunks)}
- Dur√©e totale: {total_duration:.1f}s
- Fournisseur: {provider}

{'-' * 80}
                        PROMPT UTILIS√â
{'-' * 80}

{prompt}

{'-' * 80}
                    R√âSULTAT FINAL
{'-' * 80}

{step3_result}

{'-' * 80}
                D√âTAIL DES ANALYSES PAR CHUNKS
{'-' * 80}

{combined_analyses}
"""
        
        # Statistiques
        stats1 = f"{len(text1):,} caract√®res" if text1 else "Aucun texte"
        stats2 = f"{len(text2):,} caract√®res" if text2 else "Aucun texte"
        
        # Informations de debug
        debug_info = f"""ANALYSE HYBRIDE EX√âCUT√âE
{'=' * 50}

MODE: Hybride 3 √©tapes
CONFIGURATION: {'Depuis .env' if ENV_AVAILABLE else 'Par d√©faut'}
DOMAINE: {domain}
SYNTH√àSE: {synthesis_type if enable_step3 else 'D√©sactiv√©e'}

MOD√àLES UTILIS√âS:
- √âtape 1 (Extraction): {model_step1}
- √âtape 2 (Fusion): {model_step2}
- √âtape 3 (Synth√®se): {model_step3 if enable_step3 else 'N/A'}

PERFORMANCE:
- Chunks trait√©s: {len(chunks)}
- Dur√©e totale: {total_duration:.1f}s
- Score efficacit√©: {len(chunks) / total_duration * 60:.1f} chunks/min

ARCHITECTURE:
‚úì Extraction rapide ‚Üí {model_step1}
‚úì Fusion intelligente ‚Üí {model_step2}
{'‚úì' if enable_step3 else '‚óã'} Synth√®se premium ‚Üí {model_step3 if enable_step3 else 'D√©sactiv√©e'}"""
        
        # Rapport d√©taill√©
        analysis_report = f"""RAPPORT D'ANALYSE HYBRIDE

√âtapes ex√©cut√©es:
1. Extraction par chunks: {step1_duration:.1f}s
2. Fusion et harmonisation: {step2_duration:.1f}s
3. Synth√®se narrative: {step3_duration:.1f}s ({'Ex√©cut√©e' if enable_step3 else 'Non ex√©cut√©e'})

Configuration: {'Depuis .env' if ENV_AVAILABLE else 'Par d√©faut'}
Chunks trait√©s: {len(chunks)}
Efficacit√©: {len(chunks) / total_duration * 60:.1f} chunks/min"""
        
        return (formatted_result, stats1, stats2, debug_info, analysis_report)
        
    except Exception as e:
        error_msg = f"ERREUR ANALYSE HYBRIDE\n\n{str(e)}"
        return (error_msg, "", "", error_msg, "Erreur")

def analyze_classic_mode(text1, text2, prompt, model, provider, temperature, top_p, max_tokens,
                        ollama_url, runpod_endpoint, runpod_token):
    """Analyse classique en utilisant VOS modules existants."""
    
    track_activity()
    
    # Si .env disponible et param√®tres manquants, utiliser .env
    if ENV_AVAILABLE:
        try:
            config = get_api_config()
            if not ollama_url and provider == "Ollama distant":
                ollama_url = config.get("ollama_remote_url", ollama_url)
            if provider == "RunPod.io":
                if not runpod_endpoint:
                    runpod_endpoint = config.get("runpod_endpoint", "")
                if not runpod_token:
                    runpod_token = config.get("runpod_token", "")
        except:
            pass
    
    # Pr√©paration du texte
    if text1 and text2:
        full_text = f"=== DOCUMENT 1 ===\n{text1}\n\n=== DOCUMENT 2 ===\n{text2}"
    else:
        full_text = text1 or text2
    
    if not full_text:
        return ("ERREUR: Aucun texte fourni", "", "", "Aucun texte", "")
    
    try:
        # Utiliser directement vos fonctions existantes
        if provider == "RunPod.io":
            result = generate_with_runpod(
                model=model,
                system_prompt=prompt,
                user_text=full_text,
                num_ctx=8192,
                num_predict=max_tokens,
                temperature=temperature,
                endpoint=runpod_endpoint,
                token=runpod_token
            )
        else:
            result = generate_with_ollama(
                model=model,
                system_prompt=prompt,
                user_text=full_text,
                num_ctx=8192,
                num_predict=max_tokens,
                temperature=temperature,
                ollama_url=ollama_url
            )
        
        # Formatage
        current_time = datetime.now().strftime("%d/%m/%Y √† %H:%M:%S")
        formatted_result = f"""{'=' * 80}
                    ANALYSE JURIDIQUE CLASSIQUE
{'=' * 80}

HORODATAGE: {current_time}
MOD√àLE: {model}
FOURNISSEUR: {provider}
MODE: Analyse directe
TEXTE: {len(full_text):,} caract√®res
CONFIGURATION: {'Depuis .env' if ENV_AVAILABLE else 'Par d√©faut'}

{'-' * 80}
                        PROMPT UTILIS√â
{'-' * 80}

{prompt}

{'-' * 80}
                    R√âSULTAT DE L'ANALYSE
{'-' * 80}

{result}
"""
        
        stats1 = f"{len(text1):,} caract√®res" if text1 else "Aucun texte"
        stats2 = f"{len(text2):,} caract√®res" if text2 else "Aucun texte"
        
        debug_info = f"""ANALYSE CLASSIQUE EX√âCUT√âE

Mode: Direct (sans chunks)
Mod√®le: {model}
Provider: {provider}
Texte: {len(full_text):,} caract√®res
Configuration: {'Depuis .env' if ENV_AVAILABLE else 'Par d√©faut'}"""
        
        return (formatted_result, stats1, stats2, debug_info, "Mode classique")
        
    except Exception as e:
        error_msg = f"ERREUR ANALYSE CLASSIQUE\n\n{str(e)}"
        return (error_msg, "", "", error_msg, "Erreur")

def fallback_analysis(text1, text2, prompt):
    """Analyse de fallback si modules non disponibles."""
    return (
        "‚ö†Ô∏è MODULES D'ANALYSE NON DISPONIBLES\n\nV√©rifiez l'installation des d√©pendances.",
        f"{len(text1) if text1 else 0:,} caract√®res",
        f"{len(text2) if text2 else 0:,} caract√®res",
        "Mode fallback - modules manquants",
        "Erreur: modules non disponibles"
    )

# ========================================
# FONCTIONS CALLBACK ADAPT√âES
# ========================================

def on_provider_change(provider):
    """Gestion du changement de fournisseur."""
    app_state["current_provider"] = provider
    
    ollama_visible = provider == "Ollama distant"
    runpod_visible = provider == "RunPod.io"
    
    if provider == "Ollama local":
        status = "‚úÖ Ollama local configur√©"
        url_value = ""
        runpod_manager.disable_auto_stop()
    elif provider == "Ollama distant":
        if ENV_AVAILABLE:
            try:
                config = get_api_config()
                url_value = config.get("ollama_remote_url", app_state["ollama_url"])
                status = f"üåê Ollama distant: {url_value or 'Configurez dans .env'}"
            except:
                url_value = app_state["ollama_url"]
                status = f"üåê Ollama distant: {url_value}"
        else:
            url_value = app_state["ollama_url"]
            status = f"üåê Ollama distant: {url_value}"
        runpod_manager.disable_auto_stop()
    else:
        url_value = ""
        if ENV_AVAILABLE:
            try:
                config = get_api_config()
                configured = bool(config.get("runpod_endpoint") and config.get("runpod_token"))
                status = f"‚òÅÔ∏è RunPod: {'‚úÖ Configur√© dans .env' if configured else '‚ùå Configurez dans .env'}"
            except:
                status = "‚òÅÔ∏è RunPod - Configurez endpoint et token"
        else:
            status = "‚òÅÔ∏è RunPod - Configurez endpoint et token"
    
    return (
        gr.update(visible=ollama_visible, value=url_value),
        gr.update(visible=runpod_visible, value=""),
        gr.update(visible=runpod_visible, value=""),
        gr.update(visible=runpod_visible),
        gr.update(visible=runpod_visible),
        gr.update(value=status)
    )

def test_connection_real(provider, ollama_url, runpod_endpoint, runpod_token):
    """Test de connexion r√©el en utilisant VOS fonctions."""
    track_activity()
    
    if not MODULES_AVAILABLE:
        return (
            gr.update(choices=["mistral:7b-instruct", "llama3:latest"], value="mistral:7b-instruct"),
            gr.update(value="‚ö†Ô∏è Mode d√©grad√© - Modules non disponibles")
        )
    
    try:
        # Si .env disponible et param√®tres manquants, utiliser .env
        if ENV_AVAILABLE:
            try:
                config = get_api_config()
                if not ollama_url and provider == "Ollama distant":
                    ollama_url = config.get("ollama_remote_url", ollama_url)
                if provider == "RunPod.io":
                    if not runpod_endpoint:
                        runpod_endpoint = config.get("runpod_endpoint", "")
                    if not runpod_token:
                        runpod_token = config.get("runpod_token", "")
            except:
                pass
        
        # Utiliser VOTRE fonction de test
        result = test_connection(provider, ollama_url, runpod_endpoint, runpod_token)
        
        if "r√©ussie" in result or "Connexion" in result:
            # R√©cup√©rer les mod√®les selon le provider
            if provider in ["Ollama local", "Ollama distant"]:
                url = ollama_url if provider == "Ollama distant" else "http://localhost:11434"
                models = get_ollama_models(url)
            else:
                # Mod√®les RunPod par d√©faut
                models = [
                    "meta-llama/Llama-3.1-70B-Instruct",
                    "mistralai/Mistral-7B-Instruct-v0.3",
                    "NousResearch/Nous-Hermes-2-Yi-34B"
                ]
            
            app_state["models_list"] = models
            suffix = " (.env)" if ENV_AVAILABLE else ""
            return (
                gr.update(choices=models, value=models[0]),
                gr.update(value=f"‚úÖ {result} - {len(models)} mod√®les{suffix}")
            )
        else:
            return (
                gr.update(),
                gr.update(value=f"‚ùå {result}")
            )
    except Exception as e:
        return (
            gr.update(),
            gr.update(value=f"‚ùå Erreur: {str(e)}")
        )

def configure_runpod_autostop(runpod_endpoint, runpod_token, timeout_minutes):
    """Configure l'arr√™t automatique RunPod."""
    
    # Si .env disponible et param√®tres manquants, utiliser .env
    if ENV_AVAILABLE and (not runpod_endpoint or not runpod_token):
        try:
            config = get_api_config()
            runpod_endpoint = config.get("runpod_endpoint", runpod_endpoint)
            runpod_token = config.get("runpod_token", runpod_token)
        except:
            pass
    
    if runpod_endpoint and runpod_token:
        message = runpod_manager.configure_auto_stop(runpod_endpoint, runpod_token, timeout_minutes)
        return gr.update(value=f"‚úÖ {message}")
    else:
        return gr.update(value="‚ùå Endpoint et token requis")

def select_prompt(prompt_name):
    """S√©lection d'un prompt en utilisant VOTRE gestionnaire."""
    if not MODULES_AVAILABLE:
        return gr.update(), gr.update(value="‚ùå Gestionnaire de prompts non disponible")
    
    try:
        content = get_prompt_content(prompt_name)
        if content:
            return (
                gr.update(value=content),
                gr.update(value=f"‚úÖ Prompt '{prompt_name}' charg√©")
            )
        else:
            return (
                gr.update(),
                gr.update(value=f"‚ùå Prompt '{prompt_name}' non trouv√©")
            )
    except Exception as e:
        return (
            gr.update(),
            gr.update(value=f"‚ùå Erreur: {str(e)}")
        )

def process_file_real(file_path, clean_text=True, anonymize=False):
    """Traitement de fichier en utilisant VOS fonctions."""
    track_activity()
    
    if not file_path:
        return "Aucun fichier", "0 caract√®res", ""
    
    if not MODULES_AVAILABLE:
        return "‚ö†Ô∏è Modules non disponibles", "Simulation", "Texte simul√©"
    
    try:
        # Utiliser VOTRE fonction de traitement
        message, stats, text, file_type, anon_report = process_file_to_text(
            file_path, clean_text, anonymize, force_ocr=False
        )
        
        if "‚ùå" in message:
            return message, "Erreur", ""
        
        return message, stats, text
    except Exception as e:
        return f"‚ùå Erreur: {str(e)}", "Erreur", ""

def clear_all_fields():
    """Nettoie tous les champs."""
    track_activity()
    if MODULES_AVAILABLE:
        default_prompt = get_prompt_content("Analyse juridique hybride") or DEFAULT_PROMPT_TEXT
    else:
        default_prompt = "Tu es un expert juridique. Analyse ce document en d√©tail."
    
    return (
        "",  # text1
        "",  # text2
        "",  # result
        "",  # stats1
        "",  # stats2
        "",  # debug
        default_prompt,  # prompt
        "üßπ Champs nettoy√©s",  # status
        ""   # analysis_report
    )

# ========================================
# INTERFACE GRADIO ADAPT√âE √Ä VOS MODULES + .ENV
# ========================================

def create_hybrid_interface():
    """Cr√©e l'interface Gradio en utilisant VOS modules existants + .env."""
    
    load_config()
    
    with gr.Blocks(
        title="OCR Juridique - Mode Hybride + .env",
        theme=gr.themes.Soft()
    ) as demo:
        
        gr.Markdown(f"""
        # üìö OCR Juridique - Mode Hybride v8.4
        
        **Analyse juridique s√©curis√©e** avec architecture 3 √©tapes et gestion .env
        
        üìÑ **√âtape 1** : Extraction chunks ‚Üí Mod√®le rapide (√©conomique)  
        üîÄ **√âtape 2** : Fusion + harmonisation ‚Üí Mod√®le contexte large  
        ‚ú® **√âtape 3** : Synth√®se narrative ‚Üí Mod√®le qualit√© r√©dactionnelle
        
        **√âtat** : {'‚úÖ Modules + .env disponibles' if MODULES_AVAILABLE and ENV_AVAILABLE else '‚ö†Ô∏è Configuration limit√©e'}
        """)
        
        with gr.Tabs():
            
            # ======= CONFIGURATION =======
            with gr.Tab("üîß Configuration"):
                with gr.Row():
                    with gr.Column():
                        provider = gr.Radio(
                            choices=["Ollama local", "Ollama distant", "RunPod.io"],
                            value="Ollama local",
                            label="Fournisseur IA"
                        )
                        
                        ollama_url = gr.Textbox(
                            label="URL Ollama distant",
                            value=app_state["ollama_url"],
                            visible=False
                        )
                        
                        runpod_endpoint = gr.Textbox(
                            label="Endpoint RunPod",
                            visible=False
                        )
                        
                        runpod_token = gr.Textbox(
                            label="Token RunPod",
                            type="password",
                            visible=False
                        )
                        
                        # Configuration arr√™t automatique RunPod
                        with gr.Group(visible=False) as runpod_autostop_group:
                            gr.Markdown("### ‚è±Ô∏è Arr√™t automatique")
                            autostop_timeout = gr.Slider(
                                minimum=5, maximum=60, value=15, step=5,
                                label="Inactivit√© (minutes)"
                            )
                            configure_autostop_btn = gr.Button(
                                "‚öôÔ∏è Configurer arr√™t auto", 
                                variant="secondary"
                            )
                        
                        test_btn = gr.Button("üîç Tester la connexion", variant="primary")
                        
                        status_msg = gr.Textbox(
                            label="Statut",
                            value="Pr√™t" if MODULES_AVAILABLE else "Modules manquants",
                            interactive=False
                        )
                    
                    with gr.Column():
                        gr.Markdown("### Mod√®les par √©tape")
                        
                        model_step1 = gr.Dropdown(
                            choices=app_state["models_list"],
                            value="mistral:7b-instruct",
                            label="√âtape 1 - Extraction (rapide)",
                            allow_custom_value=True
                        )
                        
                        model_step2 = gr.Dropdown(
                            choices=app_state["models_list"],
                            value="llama3:latest",
                            label="√âtape 2 - Fusion (contexte large)",
                            allow_custom_value=True
                        )
                        
                        model_step3 = gr.Dropdown(
                            choices=app_state["models_list"],
                            value="llama3:latest",
                            label="√âtape 3 - Synth√®se (qualit√©)",
                            allow_custom_value=True
                        )
                        
                        with gr.Row():
                            temperature = gr.Slider(0, 2, value=0.2, step=0.1, label="Temp√©rature")
                            top_p = gr.Slider(0, 1, value=0.9, step=0.1, label="Top-p")
                            max_tokens = gr.Slider(500, 8000, value=2000, step=500, label="Max tokens")
            
            # ======= GESTION .ENV =======
            with gr.Tab("üîê Gestion .env"):
                if ENV_AVAILABLE:
                    gr.Markdown("""
                    ### üîê Configuration s√©curis√©e des cl√©s API
                    
                    G√©rez vos cl√©s API via le fichier `.env` pour plus de s√©curit√©.
                    """)
                    
                    # Interface de gestion
                    env_selector, env_value, env_status, env_report = create_env_management_interface()
                    
                    # Boutons suppl√©mentaires
                    with gr.Row():
                        reload_env_btn = gr.Button("üîÑ Recharger .env", variant="secondary")
                        status_env_btn = gr.Button("üìä Statut complet", variant="primary")
                    
                    def reload_env_config():
                        try:
                            env_manager.load_env()
                            return "‚úÖ Configuration .env recharg√©e"
                        except Exception as e:
                            return f"‚ùå Erreur: {e}"
                    
                    def get_env_status():
                        try:
                            return env_manager.status_report()
                        except Exception as e:
                            return f"‚ùå Erreur: {e}"
                    
                    reload_env_btn.click(reload_env_config, outputs=[env_status])
                    status_env_btn.click(get_env_status, outputs=[env_report])
                    
                else:
                    gr.Markdown("""
                    ‚ö†Ô∏è **Gestionnaire .env non disponible**
                    
                    Pour activer la gestion s√©curis√©e des cl√©s API:
                    
                    1. **T√©l√©chargez** le fichier `env_manager.py`
                    2. **Placez-le** dans le m√™me dossier que ce script
                    3. **Cr√©ez** un fichier `.env` avec vos cl√©s:
                    
                    ```bash
                    # Exemple de fichier .env
                    RUNPOD_ENDPOINT=https://api.runpod.ai/v2/votre-endpoint
                    RUNPOD_TOKEN=votre-token-runpod
                    OLLAMA_REMOTE_URL=http://votre-serveur:11434
                    ANTHROPIC_API_KEY=sk-ant-votre-cle
                    DEFAULT_CHUNK_SIZE=3000
                    ```
                    
                    4. **Red√©marrez** l'application
                    """)
            
            # ======= DOCUMENTS =======
            with gr.Tab("üìÑ Documents"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### Document 1")
                        file1 = gr.File(label="Fichier 1", file_types=[".pdf", ".txt", ".docx"])
                        with gr.Row():
                            clean1 = gr.Checkbox(label="Nettoyer", value=True)
                            anon1 = gr.Checkbox(label="Anonymiser", value=False)
                        text1 = gr.Textbox(label="Texte 1", lines=10)
                        stats1 = gr.Textbox(label="Statistiques 1", interactive=False)
                    
                    with gr.Column():
                        gr.Markdown("### Document 2")
                        file2 = gr.File(label="Fichier 2", file_types=[".pdf", ".txt", ".docx"])
                        with gr.Row():
                            clean2 = gr.Checkbox(label="Nettoyer", value=True)
                            anon2 = gr.Checkbox(label="Anonymiser", value=False)
                        text2 = gr.Textbox(label="Texte 2", lines=10)
                        stats2 = gr.Textbox(label="Statistiques 2", interactive=False)
                
                clear_btn = gr.Button("üóëÔ∏è Nettoyer", variant="stop")
            
            # ======= ANALYSE =======
            with gr.Tab("üîç Analyse"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### Mode d'analyse")
                        analysis_mode = gr.Radio(
                            choices=["Hybride 3 √©tapes", "Classique direct"],
                            value="Hybride 3 √©tapes",
                            label="Mode"
                        )
                        
                        # Configuration hybride
                        with gr.Group() as hybrid_config:
                            chunk_size = gr.Slider(
                                1000, 5000, value=DEFAULT_CONFIG["chunk_size"], step=500,
                                label="Taille chunks"
                            )
                            chunk_overlap = gr.Slider(
                                0, 500, value=DEFAULT_CONFIG["chunk_overlap"], step=50,
                                label="Chevauchement"
                            )
                            
                            domain = gr.Dropdown(
                                choices=["Aucun"] + list(SUPPORTED_DOMAINS.values()),
                                value="Aucun",
                                label="Domaine sp√©cialis√©"
                            )
                            
                            enable_step3 = gr.Checkbox(
                                label="Synth√®se narrative premium (√©tape 3)",
                                value=True
                            )
                            
                            synthesis_type = gr.Dropdown(
                                choices=list(SYNTHESIS_TYPES.values()),
                                value="Synth√®se ex√©cutive",
                                label="Type de synth√®se"
                            )
                        
                        # Prompts pr√©d√©finis
                        gr.Markdown("### Prompts")
                        if MODULES_AVAILABLE:
                            available_prompts = get_all_prompts_for_dropdown()
                        else:
                            available_prompts = ["Analyse juridique par d√©faut"]
                        
                        prompt_selector = gr.Dropdown(
                            choices=available_prompts,
                            label="Prompts pr√©d√©finis"
                        )
                        select_prompt_btn = gr.Button("üìù Charger")
                        prompt_status = gr.Textbox(label="Statut", interactive=False)
                    
                    with gr.Column(scale=2):
                        if MODULES_AVAILABLE:
                            default_prompt = get_prompt_content("Analyse juridique hybride") or DEFAULT_PROMPT_TEXT
                        else:
                            default_prompt = "Tu es un expert juridique. Analyse ce document en d√©tail."
                        
                        prompt_text = gr.Textbox(
                            label="Prompt d'analyse",
                            lines=10,
                            value=default_prompt,
                            placeholder="D√©crivez l'analyse souhait√©e..."
                        )
                        
                        analyze_btn = gr.Button(
                            "üöÄ Lancer l'analyse", 
                            variant="primary", 
                            size="lg"
                        )
            
            # ======= R√âSULTATS =======
            with gr.Tab("üìä R√©sultats"):
                result_text = gr.Textbox(
                    label="R√©sultat de l'analyse",
                    lines=25,
                    show_copy_button=True
                )
                
                with gr.Row():
                    with gr.Column():
                        with gr.Accordion("üìã Rapport d'analyse", open=False):
                            analysis_report = gr.Textbox(
                                label="D√©tails", lines=8, interactive=False
                            )
                    with gr.Column():
                        with gr.Accordion("üîß Debug", open=False):
                            debug_info = gr.Textbox(
                                label="Informations techniques", lines=8, interactive=False
                            )
        
        # ========================================
        # √âV√âNEMENTS
        # ========================================
        
        # Configuration
        provider.change(
            on_provider_change,
            inputs=[provider],
            outputs=[ollama_url, runpod_endpoint, runpod_token, runpod_autostop_group, configure_autostop_btn, status_msg]
        )
        
        test_btn.click(
            test_connection_real,
            inputs=[provider, ollama_url, runpod_endpoint, runpod_token],
            outputs=[model_step1, status_msg]
        )
        
        # Configuration arr√™t automatique RunPod
        configure_autostop_btn.click(
            configure_runpod_autostop,
            inputs=[runpod_endpoint, runpod_token, autostop_timeout],
            outputs=[status_msg]
        )
        
        # Gestion fichiers
        def handle_file1(file):
            if file:
                message, stats, text = process_file_real(file.name, clean1.value, anon1.value)
                return message, stats, text
            return "", "0 caract√®res", ""
        
        def handle_file2(file):
            if file:
                message, stats, text = process_file_real(file.name, clean2.value, anon2.value)
                return stats, text
            return "0 caract√®res", ""
        
        file1.change(handle_file1, inputs=[file1], outputs=[status_msg, stats1, text1])
        file2.change(handle_file2, inputs=[file2], outputs=[stats2, text2])
        
        # Prompts
        select_prompt_btn.click(
            select_prompt,
            inputs=[prompt_selector],
            outputs=[prompt_text, prompt_status]
        )
        
        # Affichage conditionnel config hybride
        def toggle_config(mode):
            return gr.update(visible=(mode == "Hybride 3 √©tapes"))
        
        analysis_mode.change(
            toggle_config,
            inputs=[analysis_mode],
            outputs=[hybrid_config]
        )
        
        # Analyse principale
        def route_analysis(mode, text1, text2, prompt, 
                          model_step1, model_step2, model_step3,
                          provider, temperature, top_p, max_tokens,
                          chunk_size, chunk_overlap, domain, synthesis_type, enable_step3,
                          ollama_url, runpod_endpoint, runpod_token):
            
            if mode == "Hybride 3 √©tapes":
                return analyze_hybrid_mode(
                    text1, text2, prompt, model_step1, model_step2, model_step3,
                    provider, temperature, top_p, max_tokens,
                    chunk_size, chunk_overlap, domain, synthesis_type, enable_step3,
                    ollama_url, runpod_endpoint, runpod_token
                )
            else:
                return analyze_classic_mode(
                    text1, text2, prompt, model_step1, provider, 
                    temperature, top_p, max_tokens,
                    ollama_url, runpod_endpoint, runpod_token
                )
        
        analyze_btn.click(
            route_analysis,
            inputs=[
                analysis_mode, text1, text2, prompt_text,
                model_step1, model_step2, model_step3,
                provider, temperature, top_p, max_tokens,
                chunk_size, chunk_overlap, domain, synthesis_type, enable_step3,
                ollama_url, runpod_endpoint, runpod_token
            ],
            outputs=[result_text, stats1, stats2, debug_info, analysis_report]
        )
        
        # Nettoyage
        clear_btn.click(
            clear_all_fields,
            outputs=[text1, text2, result_text, stats1, stats2, debug_info, 
                    prompt_text, status_msg, analysis_report]
        )
        
        # Mise √† jour des mod√®les apr√®s test de connexion
        def update_all_models(provider, ollama_url, runpod_endpoint, runpod_token):
            dropdown_result, status_result = test_connection_real(provider, ollama_url, runpod_endpoint, runpod_token)
            return dropdown_result, dropdown_result, dropdown_result, status_result
        
        test_btn.click(
            update_all_models,
            inputs=[provider, ollama_url, runpod_endpoint, runpod_token],
            outputs=[model_step1, model_step2, model_step3, status_msg]
        )
    
    return demo

# ========================================
# FONCTION BUILD_UI ADAPT√âE
# ========================================

def build_ui():
    """Point d'entr√©e pour main_ocr.py - utilise VOS modules + .env."""
    print("Construction interface hybride avec modules existants + .env...")
    print(f"Modules disponibles: {MODULES_AVAILABLE}")
    print(f"Gestion .env: {'Disponible' if ENV_AVAILABLE else 'Non disponible'}")
    
    if MODULES_AVAILABLE:
        print("‚úÖ Utilisation de vos modules:")
        print("  - ai_providers.py")
        print("  - chunck_analysis.py") 
        print("  - prompt_manager.py")
        print("  - processing_pipeline.py")
        print("  - config.py")
        
        if ENV_AVAILABLE:
            print("  - env_manager.py (nouveau)")
            try:
                config = get_api_config()
                print(f"üîë Config .env: Ollama={bool(config.get('ollama_url'))}, RunPod={bool(config.get('runpod_endpoint'))}")
            except:
                print("‚ö†Ô∏è Erreur lecture .env")
    else:
        print("‚ö†Ô∏è Certains modules manquants - Mode d√©grad√©")
    
    return create_hybrid_interface()

# ========================================
# LANCEMENT DIRECT
# ========================================

if __name__ == "__main__":
    print("Interface hybride - Test direct avec modules existants + .env")
    
    # Afficher config .env si disponible
    if ENV_AVAILABLE:
        try:
            print("\n" + "="*50)
            print("CONFIGURATION .ENV")
            print("="*50)
            print(env_manager.status_report())
            print("="*50 + "\n")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur .env: {e}")
    
    demo = create_hybrid_interface()
    
    # Utiliser config .env pour lancement si disponible, avec port dynamique
    if ENV_AVAILABLE:
        try:
            config = get_app_config()
            print(f"üåê Lancement: {config['host']}:{config.get('port', 'auto')}")
            
            # Si port sp√©cifi√© dans .env, l'utiliser, sinon laisser Gradio choisir
            if config.get('port'):
                demo.launch(
                    server_name=config["host"],
                    server_port=config["port"],
                    debug=config["debug"]
                )
            else:
                # Port automatique
                demo.launch(
                    server_name=config["host"],
                    debug=config["debug"]
                )
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur config .env: {e}, lancement automatique")
            demo.launch()  # Port automatique
    else:
        print("üåê Lancement avec port automatique")
        demo.launch()  # Port automatique
